{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "unzipping"
      ],
      "metadata": {
        "id": "hQLQt6ftrMft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkq4iTtOp3nf",
        "outputId": "97241bf2-1a5d-4bc9-c756-de9d7e587fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file '/content/drive/MyDrive/real_cifake_preds.json' is not a valid zip file or is corrupted.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- 1. Set your paths ---\n",
        "\n",
        "# This is the name of the file you downloaded from the hackathon.\n",
        "# (e.g., \"Synergy25_dataset.zip\")\n",
        "zip_file_path = '/content/drive/MyDrive/real_cifake_preds.json'\n",
        "\n",
        "# This is the name of the folder where you want all the files to go.\n",
        "# (e.t., \"dataset/\")\n",
        "destination_folder = 'hackathon_dataset'\n",
        "\n",
        "# --- 2. Create the destination folder if it doesn't exist ---\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "    print(f\"Created directory: {destination_folder}\")\n",
        "\n",
        "# --- 3. Unzip the file ---\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        print(f\"Unzipping '{zip_file_path}'...\")\n",
        "        zip_ref.extractall(destination_folder)\n",
        "        print(f\"Successfully unzipped all files to '{destination_folder}'\")\n",
        "\n",
        "        # Optional: List the files you unzipped\n",
        "        print(\"\\nUnzipped contents:\")\n",
        "        print(zip_ref.namelist())\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file '{zip_file_path}' is not a valid zip file or is corrupted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{zip_file_path}' was not found.\")\n",
        "    print(\"Please make sure the file is in the same directory as this script, or provide the full path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "0xZekcdFuNKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import collections\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# It should contain the 5 subfolders ('real images', 'fake images', etc.)\n",
        "BASE_DATA_DIR = 'hackathon_dataset'\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/real_cifake_images')\n",
        "FAKE_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/fake_cifake_images')\n",
        "TEST_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/test')\n",
        "REAL_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/real_cifake_preds.json', '/content/drive/MyDrive/real_cifake_preds.json') # Assuming file is named this\n",
        "FAKE_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/fake_cifake_preds.json', '/content/drive/MyDrive/fake_cifake_preds.json') # Assuming file is named this\n",
        "\n",
        "print(\"--- Starting Dataset Verification ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 1: File Count Sanity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 1: File Count Sanity Check]\")\n",
        "try:\n",
        "    num_real_images = len(os.listdir(REAL_IMG_DIR))\n",
        "    num_fake_images = len(os.listdir(FAKE_IMG_DIR))\n",
        "    num_test_images = len(os.listdir(TEST_IMG_DIR))\n",
        "\n",
        "    print(f\"Found {num_real_images} images in 'real images' folder.\")\n",
        "    print(f\"Found {num_fake_images} images in 'fake images' folder.\")\n",
        "    print(f\"Found {num_test_images} images in 'test image' folder.\")\n",
        "\n",
        "    if num_real_images == 1000 and num_fake_images == 1000:\n",
        "        print(\"✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Image counts do not match the expected 1000/1000 split.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A folder was not found. Please check your paths. Details: {e}\")\n",
        "    exit() # Stop the script if basic folders are missing\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 2: The \"Imperfect Model\" Check (JSON Analysis)\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 2: JSON Prediction Analysis]\")\n",
        "try:\n",
        "    with open(REAL_JSON_PATH, 'r') as f:\n",
        "        real_json_data = json.load(f)\n",
        "    with open(FAKE_JSON_PATH, 'r') as f:\n",
        "        fake_json_data = json.load(f)\n",
        "\n",
        "    # Count predictions in the JSON for REAL images\n",
        "    real_json_counts = collections.Counter(item['prediction'] for item in real_json_data)\n",
        "    print(\"Proprietary model's predictions on REAL images:\")\n",
        "    print(f\"  - Predicted 'real': {real_json_counts.get('real', 0)}\")\n",
        "    print(f\"  - Predicted 'fake': {real_json_counts.get('fake', 0)}\")\n",
        "\n",
        "    # Count predictions in the JSON for FAKE images\n",
        "    fake_json_counts = collections.Counter(item['prediction'] for item in fake_json_data)\n",
        "    print(\"Proprietary model's predictions on FAKE images:\")\n",
        "    print(f\"  - Predicted 'fake': {fake_json_counts.get('fake', 0)}\")\n",
        "    print(f\"  - Predicted 'real': {fake_json_counts.get('real', 0)}\")\n",
        "\n",
        "    # --- The CRITICAL VERDICT ---\n",
        "    if real_json_counts.get('fake', 0) == 0 and fake_json_counts.get('real', 0) == 0:\n",
        "        print(\"✅ STATUS: The proprietary model is 'perfect' on the training set.\")\n",
        "        print(\"   Our task is a standard, balanced binary classification.\")\n",
        "    else:\n",
        "        print(\"⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\")\n",
        "        print(\"   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A JSON file was not found. Please check your JSON file names and paths. Details: {e}\")\n",
        "    exit()\n",
        "except json.JSONDecodeError:\n",
        "    print(\"❌ ERROR: Could not parse a JSON file. It might be corrupted.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 3: Image Format & Integrity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\")\n",
        "image_sizes = set()\n",
        "image_modes = set()\n",
        "corrupted_files = []\n",
        "\n",
        "def check_images(directory, num_to_check=10):\n",
        "    files = os.listdir(directory)\n",
        "    for i, filename in enumerate(files):\n",
        "        if i >= num_to_check:\n",
        "            break\n",
        "        try:\n",
        "            with Image.open(os.path.join(directory, filename)) as img:\n",
        "                image_sizes.add(img.size)\n",
        "                image_modes.add(img.mode)\n",
        "        except Exception as e:\n",
        "            corrupted_files.append(os.path.join(directory, filename))\n",
        "\n",
        "try:\n",
        "    check_images(REAL_IMG_DIR)\n",
        "    check_images(FAKE_IMG_DIR)\n",
        "\n",
        "    print(f\"Found image sizes: {image_sizes}\")\n",
        "    print(f\"Found image modes (e.g., RGB, L): {image_modes}\")\n",
        "\n",
        "    if len(image_sizes) == 1:\n",
        "        print(\"✅ STATUS: All tested images have a consistent size.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying sizes. We will need to resize them all.\")\n",
        "\n",
        "    if len(image_modes) == 1 and 'RGB' in image_modes:\n",
        "        print(\"✅ STATUS: All tested images are in consistent 'RGB' mode.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying modes (e.g., Grayscale 'L') or are not RGB.\")\n",
        "\n",
        "    if not corrupted_files:\n",
        "        print(\"✅ STATUS: No corrupted images found in the sample.\")\n",
        "    else:\n",
        "        print(f\"⚠️ WARNING: Found {len(corrupted_files)} corrupted images: {corrupted_files}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: An unexpected error occurred during image check. Details: {e}\")\n",
        "\n",
        "print(\"\\n--- Verification Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJMbRaNqp54Y",
        "outputId": "8e1e8659-0de8-408c-a64b-8014cc66b556"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Dataset Verification ---\n",
            "\n",
            "[CHECK 1: File Count Sanity Check]\n",
            "Found 1000 images in 'real images' folder.\n",
            "Found 1000 images in 'fake images' folder.\n",
            "Found 500 images in 'test image' folder.\n",
            "✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\n",
            "\n",
            "[CHECK 2: JSON Prediction Analysis]\n",
            "Proprietary model's predictions on REAL images:\n",
            "  - Predicted 'real': 976\n",
            "  - Predicted 'fake': 24\n",
            "Proprietary model's predictions on FAKE images:\n",
            "  - Predicted 'fake': 988\n",
            "  - Predicted 'real': 12\n",
            "⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\n",
            "   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\n",
            "\n",
            "[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\n",
            "Found image sizes: {(32, 32)}\n",
            "Found image modes (e.g., RGB, L): {'RGB'}\n",
            "✅ STATUS: All tested images have a consistent size.\n",
            "✅ STATUS: All tested images are in consistent 'RGB' mode.\n",
            "✅ STATUS: No corrupted images found in the sample.\n",
            "\n",
            "--- Verification Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# BASE_DATA_DIR = 'hackathon_dataset' # No longer needed with absolute paths\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = '/content/hackathon_dataset/real_cifake_images'\n",
        "FAKE_IMG_DIR = '/content/hackathon_dataset/fake_cifake_images'\n",
        "REAL_JSON_PATH = '/content/drive/MyDrive/real_cifake_preds.json'\n",
        "FAKE_JSON_PATH = '/content/drive/MyDrive/fake_cifake_preds.json'\n",
        "\n",
        "# Output file name\n",
        "OUTPUT_CSV_PATH = 'master_labels.csv'\n",
        "\n",
        "def process_data(image_dir, json_path, data_list):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and an image directory, and populates a list with\n",
        "    image paths and their corresponding target labels.\n",
        "    \"\"\"\n",
        "    print(f\"Processing data from: {os.path.basename(json_path)}\")\n",
        "\n",
        "    # --- Load the JSON prediction data ---\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            predictions = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: JSON file not found at {json_path}. Please check the path and filename.\")\n",
        "        return False\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"❌ ERROR: Could not decode JSON from {json_path}. The file might be corrupted.\")\n",
        "        return False\n",
        "\n",
        "    # --- Create a dictionary for quick lookup: {index: prediction} ---\n",
        "    prediction_map = {item['index']: item['prediction'] for item in predictions}\n",
        "\n",
        "    # --- Iterate through images and create the master list ---\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for filename in image_files:\n",
        "        # Assumes image filenames are like \"1.jpg\", \"2.png\", etc.\n",
        "        # We extract the number to use as the index.\n",
        "        try:\n",
        "            # Get the base name without extension (e.g., \"1\") and convert to integer\n",
        "            file_index = int(os.path.splitext(filename)[0])\n",
        "        except ValueError:\n",
        "            print(f\"⚠️ Warning: Could not parse index from filename '{filename}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        if file_index in prediction_map:\n",
        "            prediction_str = prediction_map[file_index]\n",
        "\n",
        "            # Encode labels: \"real\" -> 0, \"fake\" -> 1\n",
        "            target_label = 1 if prediction_str == 'fake' else 0\n",
        "\n",
        "            # Get the full path to the image\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            data_list.append({\n",
        "                'image_path': image_path,\n",
        "                'target_label': target_label\n",
        "            })\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: No prediction found in JSON for image index {file_index} ('{filename}').\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data preparation process.\"\"\"\n",
        "    print(\"--- Starting Step 1: Data Preparation ---\")\n",
        "\n",
        "    master_data_list = []\n",
        "\n",
        "    # Process the \"real\" images and their corresponding JSON predictions\n",
        "    if not process_data(REAL_IMG_DIR, REAL_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # Process the \"fake\" images and their corresponding JSON predictions\n",
        "    if not process_data(FAKE_IMG_DIR, FAKE_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # --- Convert the list to a pandas DataFrame ---\n",
        "    if not master_data_list:\n",
        "        print(\"❌ ERROR: No data was processed. The master list is empty. Halting.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(master_data_list)\n",
        "\n",
        "    # --- Shuffle the DataFrame to mix real and fake samples ---\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # --- Save the final DataFrame to a CSV file ---\n",
        "    try:\n",
        "        df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "        print(f\"\\n✅ Success! Created master dataset with {len(df)} entries.\")\n",
        "        print(f\"   Saved to '{OUTPUT_CSV_PATH}'.\")\n",
        "\n",
        "        # Display the first few rows and the class distribution\n",
        "        print(\"\\n--- Dataset Preview ---\")\n",
        "        print(df.head())\n",
        "        print(\"\\n--- Final Label Distribution ---\")\n",
        "        print(df['target_label'].value_counts())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Could not save the CSV file. Details: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMztruRp52v",
        "outputId": "28f4e782-8e25-44f1-dad9-5e7d3c5e43ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 1: Data Preparation ---\n",
            "Processing data from: real_cifake_preds.json\n",
            "Processing data from: fake_cifake_preds.json\n",
            "\n",
            "✅ Success! Created master dataset with 2000 entries.\n",
            "   Saved to 'master_labels.csv'.\n",
            "\n",
            "--- Dataset Preview ---\n",
            "                                          image_path  target_label\n",
            "0  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "1  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "2  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "3  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "4  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "\n",
            "--- Final Label Distribution ---\n",
            "target_label\n",
            "1    1012\n",
            "0     988\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "teUtKSXMIkbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model.pth'\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32 # Based on our verification step\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            # Return a dummy image and label if file is missing\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "try:\n",
        "    df = pd.read_csv(MASTER_CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found. Please run the data preparation script first.\")\n",
        "    exit()\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,       # 80% training, 20% validation\n",
        "    random_state=42,\n",
        "    stratify=df['target_label'] # CRITICAL for maintaining label distribution\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1) # Output is a single value\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss() # Handles the sigmoid activation internally\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Announce LR change manually if it happens\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4H_tZWo5p5xk",
        "outputId": "2ccf752a-8266-44ec-bc60-e5ee6b92dc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5283 | Train Accuracy: 0.7500\n",
            "  Valid Loss: 0.8017 | Valid Accuracy: 0.7275\n",
            "✅ New best model saved with validation accuracy: 0.7275\n",
            "\n",
            "--- Epoch 2/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.3482 | Train Accuracy: 0.8600\n",
            "  Valid Loss: 0.4447 | Valid Accuracy: 0.8375\n",
            "✅ New best model saved with validation accuracy: 0.8375\n",
            "\n",
            "--- Epoch 3/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3010 | Train Accuracy: 0.8900\n",
            "  Valid Loss: 0.4851 | Valid Accuracy: 0.8175\n",
            "\n",
            "--- Epoch 4/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2550 | Train Accuracy: 0.9062\n",
            "  Valid Loss: 0.5031 | Valid Accuracy: 0.8625\n",
            "✅ New best model saved with validation accuracy: 0.8625\n",
            "\n",
            "--- Epoch 5/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2207 | Train Accuracy: 0.9094\n",
            "  Valid Loss: 0.4531 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 6/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2245 | Train Accuracy: 0.9237\n",
            "  Valid Loss: 0.4137 | Valid Accuracy: 0.8375\n",
            "\n",
            "--- Epoch 7/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2178 | Train Accuracy: 0.9175\n",
            "  Valid Loss: 0.4627 | Valid Accuracy: 0.8275\n",
            "\n",
            "--- Epoch 8/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.1714 | Train Accuracy: 0.9375\n",
            "  Valid Loss: 0.4884 | Valid Accuracy: 0.8550\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 9/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1414 | Train Accuracy: 0.9519\n",
            "  Valid Loss: 0.4771 | Valid Accuracy: 0.8450\n",
            "\n",
            "--- Epoch 10/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.21it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.0963 | Train Accuracy: 0.9681\n",
            "  Valid Loss: 0.4608 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 11/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.0762 | Train Accuracy: 0.9738\n",
            "  Valid Loss: 0.4601 | Valid Accuracy: 0.8650\n",
            "✅ New best model saved with validation accuracy: 0.8650\n",
            "\n",
            "--- Epoch 12/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.0747 | Train Accuracy: 0.9731\n",
            "  Valid Loss: 0.4915 | Valid Accuracy: 0.8475\n",
            "\n",
            "--- Epoch 13/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.0562 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.4965 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 14/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.22it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.0559 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.5094 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 15/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.0392 | Train Accuracy: 0.9856\n",
            "  Valid Loss: 0.5127 | Valid Accuracy: 0.8700\n",
            "✅ New best model saved with validation accuracy: 0.8700\n",
            "\n",
            "--- Epoch 16/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.0456 | Train Accuracy: 0.9844\n",
            "  Valid Loss: 0.5680 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 17/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.0242 | Train Accuracy: 0.9925\n",
            "  Valid Loss: 0.5692 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 18/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0435 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.6004 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 19/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation:  57%|█████▋    | 4/7 [00:00<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3219666678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3219666678.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: Image not found at {img_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICTION"
      ],
      "metadata": {
        "id": "bxxt5xZ3InjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model.pth'\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction.json' # IMPORTANT: Rename this with your team name\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64 # Can be larger for inference\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract index from filename (e.g., \"501.jpg\" -> 501)\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad(): # Disable gradient calculation for speed\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Apply sigmoid and threshold at 0.5 to get final predictions\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            # Decode label: 1 -> \"fake\", 0 -> \"real\"\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "# Sort predictions by index for a clean, ordered output file\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    # Print a sample of the output\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgoyA3NPp5tg",
        "outputId": "748904a5-8259-4d5b-adee-b7cf142eb9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading model from 'best_model.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:01<00:00,  6.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 TRAINING"
      ],
      "metadata": {
        "id": "k2dux5DfzMEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model_v2.pth' # Saving to a new file to avoid overwriting the original\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set with RandomErasing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)) # TECHNIQUE 3: Stronger Augmentation\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "df = pd.read_csv(MASTER_CSV_PATH)\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['target_label']\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # TECHNIQUE 1: Increased Dropout\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # TECHNIQUE 2: Added Weight Decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWRMKqlyp5rJ",
        "outputId": "534845a1-4bc5-4bec-fd97-c928d49b735c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "\n",
            "--- Epoch 1/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.90it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5973 | Train Accuracy: 0.6969\n",
            "  Valid Loss: 1.4621 | Valid Accuracy: 0.5450\n",
            "✅ New best model saved with validation accuracy: 0.5450\n",
            "\n",
            "--- Epoch 2/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.3901 | Train Accuracy: 0.8387\n",
            "  Valid Loss: 0.7818 | Valid Accuracy: 0.7750\n",
            "✅ New best model saved with validation accuracy: 0.7750\n",
            "\n",
            "--- Epoch 3/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.57it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3014 | Train Accuracy: 0.8812\n",
            "  Valid Loss: 0.4547 | Valid Accuracy: 0.8600\n",
            "✅ New best model saved with validation accuracy: 0.8600\n",
            "\n",
            "--- Epoch 4/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.07it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2815 | Train Accuracy: 0.8988\n",
            "  Valid Loss: 0.4314 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 5/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.04it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2395 | Train Accuracy: 0.9119\n",
            "  Valid Loss: 0.3206 | Valid Accuracy: 0.8800\n",
            "✅ New best model saved with validation accuracy: 0.8800\n",
            "\n",
            "--- Epoch 6/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2215 | Train Accuracy: 0.9194\n",
            "  Valid Loss: 0.3956 | Valid Accuracy: 0.8700\n",
            "\n",
            "--- Epoch 7/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.61it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2083 | Train Accuracy: 0.9200\n",
            "  Valid Loss: 0.3659 | Valid Accuracy: 0.8525\n",
            "\n",
            "--- Epoch 8/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.07it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.1891 | Train Accuracy: 0.9275\n",
            "  Valid Loss: 0.3607 | Valid Accuracy: 0.8700\n",
            "\n",
            "--- Epoch 9/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.01it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1769 | Train Accuracy: 0.9306\n",
            "  Valid Loss: 0.3741 | Valid Accuracy: 0.8650\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 10/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.50it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.1196 | Train Accuracy: 0.9494\n",
            "  Valid Loss: 0.3175 | Valid Accuracy: 0.8725\n",
            "\n",
            "--- Epoch 11/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.58it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.1139 | Train Accuracy: 0.9581\n",
            "  Valid Loss: 0.2923 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 12/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.94it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.0900 | Train Accuracy: 0.9681\n",
            "  Valid Loss: 0.2945 | Valid Accuracy: 0.8875\n",
            "✅ New best model saved with validation accuracy: 0.8875\n",
            "\n",
            "--- Epoch 13/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.95it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.0832 | Train Accuracy: 0.9688\n",
            "  Valid Loss: 0.3196 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 14/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.39it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.0656 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3184 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 15/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.0689 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3612 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 16/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.85it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.0621 | Train Accuracy: 0.9775\n",
            "  Valid Loss: 0.3543 | Valid Accuracy: 0.8775\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 17/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.08it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.0589 | Train Accuracy: 0.9775\n",
            "  Valid Loss: 0.3421 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 18/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0646 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3406 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 19/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.33it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.0648 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.3382 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 20/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.79it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.0516 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.3353 | Valid Accuracy: 0.8875\n",
            "Learning rate reduced from 1e-05 to 1.0000000000000002e-06\n",
            "\n",
            "--- Epoch 21/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.06it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.0627 | Train Accuracy: 0.9775\n",
            "  Valid Loss: 0.3498 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 22/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.35it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0538 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.3421 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 23/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.53it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0610 | Train Accuracy: 0.9775\n",
            "  Valid Loss: 0.3320 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 24/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.91it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.0603 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3356 | Valid Accuracy: 0.8850\n",
            "Learning rate reduced from 1.0000000000000002e-06 to 1.0000000000000002e-07\n",
            "\n",
            "--- Epoch 25/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.27it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0603 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3302 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 26/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.35it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0707 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3305 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 27/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0545 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.3293 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 28/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.77it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0579 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3315 | Valid Accuracy: 0.8850\n",
            "Learning rate reduced from 1.0000000000000002e-07 to 1.0000000000000004e-08\n",
            "\n",
            "--- Epoch 29/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.21it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0632 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3446 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 30/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.29it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0384 | Train Accuracy: 0.9875\n",
            "  Valid Loss: 0.3417 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 31/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.59it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 Summary:\n",
            "  Train Loss: 0.0648 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.3373 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 32/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.75it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 Summary:\n",
            "  Train Loss: 0.0703 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.3377 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 33/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.41it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 Summary:\n",
            "  Train Loss: 0.0515 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.3359 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 34/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.45it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 Summary:\n",
            "  Train Loss: 0.0557 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.3368 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 35/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.39it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 Summary:\n",
            "  Train Loss: 0.0569 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3276 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 36/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.75it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 Summary:\n",
            "  Train Loss: 0.0518 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.3343 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 37/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.41it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 Summary:\n",
            "  Train Loss: 0.0596 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3334 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 38/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.45it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 Summary:\n",
            "  Train Loss: 0.0620 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.3371 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 39/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 Summary:\n",
            "  Train Loss: 0.0541 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.3329 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 40/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.78it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 Summary:\n",
            "  Train Loss: 0.0476 | Train Accuracy: 0.9856\n",
            "  Valid Loss: 0.3335 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 41/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Summary:\n",
            "  Train Loss: 0.0467 | Train Accuracy: 0.9888\n",
            "  Valid Loss: 0.3293 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 42/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.54it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 Summary:\n",
            "  Train Loss: 0.0458 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.3324 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 43/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.54it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 Summary:\n",
            "  Train Loss: 0.0460 | Train Accuracy: 0.9850\n",
            "  Valid Loss: 0.3299 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 44/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.69it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 Summary:\n",
            "  Train Loss: 0.0574 | Train Accuracy: 0.9850\n",
            "  Valid Loss: 0.3277 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 45/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 Summary:\n",
            "  Train Loss: 0.0503 | Train Accuracy: 0.9838\n",
            "  Valid Loss: 0.3257 | Valid Accuracy: 0.8900\n",
            "✅ New best model saved with validation accuracy: 0.8900\n",
            "\n",
            "--- Epoch 46/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.45it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 Summary:\n",
            "  Train Loss: 0.0585 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.3234 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 47/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.43it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 Summary:\n",
            "  Train Loss: 0.0624 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3297 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 48/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.64it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 Summary:\n",
            "  Train Loss: 0.0638 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3362 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 49/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.34it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 Summary:\n",
            "  Train Loss: 0.0624 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.3296 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 50/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.39it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 Summary:\n",
            "  Train Loss: 0.0573 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.3375 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 51/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51 Summary:\n",
            "  Train Loss: 0.0515 | Train Accuracy: 0.9850\n",
            "  Valid Loss: 0.3265 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 52/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.63it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52 Summary:\n",
            "  Train Loss: 0.0491 | Train Accuracy: 0.9838\n",
            "  Valid Loss: 0.3385 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 53/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53 Summary:\n",
            "  Train Loss: 0.0541 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3414 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 54/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.49it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54 Summary:\n",
            "  Train Loss: 0.0598 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.3515 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 55/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.47it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55 Summary:\n",
            "  Train Loss: 0.0658 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.3416 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 56/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.65it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56 Summary:\n",
            "  Train Loss: 0.0603 | Train Accuracy: 0.9775\n",
            "  Valid Loss: 0.3360 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 57/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.31it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57 Summary:\n",
            "  Train Loss: 0.0569 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.3303 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 58/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.68it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58 Summary:\n",
            "  Train Loss: 0.0575 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3420 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 59/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59 Summary:\n",
            "  Train Loss: 0.0657 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3503 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 60/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.64it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60 Summary:\n",
            "  Train Loss: 0.0557 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.3465 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 61/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.43it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61 Summary:\n",
            "  Train Loss: 0.0503 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.3392 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 62/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62 Summary:\n",
            "  Train Loss: 0.0416 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.3325 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 63/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.51it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63 Summary:\n",
            "  Train Loss: 0.0637 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.3392 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 64/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.68it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64 Summary:\n",
            "  Train Loss: 0.0520 | Train Accuracy: 0.9838\n",
            "  Valid Loss: 0.3318 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 65/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65 Summary:\n",
            "  Train Loss: 0.0641 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.3334 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 66/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.43it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66 Summary:\n",
            "  Train Loss: 0.0594 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3323 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 67/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.57it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67 Summary:\n",
            "  Train Loss: 0.0535 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.3273 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 68/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.34it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68 Summary:\n",
            "  Train Loss: 0.0611 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3295 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 69/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.95it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69 Summary:\n",
            "  Train Loss: 0.0579 | Train Accuracy: 0.9800\n",
            "  Valid Loss: 0.3412 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 70/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.44it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70 Summary:\n",
            "  Train Loss: 0.0648 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3324 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 71/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.53it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71 Summary:\n",
            "  Train Loss: 0.0449 | Train Accuracy: 0.9869\n",
            "  Valid Loss: 0.3414 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 72/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.66it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72 Summary:\n",
            "  Train Loss: 0.0543 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3269 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 73/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.44it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73 Summary:\n",
            "  Train Loss: 0.0588 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3321 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 74/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74 Summary:\n",
            "  Train Loss: 0.0669 | Train Accuracy: 0.9750\n",
            "  Valid Loss: 0.3284 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 75/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.61it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75 Summary:\n",
            "  Train Loss: 0.0552 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3334 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 76/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.63it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76 Summary:\n",
            "  Train Loss: 0.0506 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3409 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 77/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.39it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77 Summary:\n",
            "  Train Loss: 0.0602 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3313 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 78/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78 Summary:\n",
            "  Train Loss: 0.0627 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.3337 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 79/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79 Summary:\n",
            "  Train Loss: 0.0592 | Train Accuracy: 0.9800\n",
            "  Valid Loss: 0.3402 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 80/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.64it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80 Summary:\n",
            "  Train Loss: 0.0613 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3342 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 81/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.32it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81 Summary:\n",
            "  Train Loss: 0.0551 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3358 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 82/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.57it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82 Summary:\n",
            "  Train Loss: 0.0657 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3306 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 83/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83 Summary:\n",
            "  Train Loss: 0.0507 | Train Accuracy: 0.9856\n",
            "  Valid Loss: 0.3384 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 84/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.62it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84 Summary:\n",
            "  Train Loss: 0.0588 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3315 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 85/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.52it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85 Summary:\n",
            "  Train Loss: 0.0544 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.3354 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 86/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.78it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  8.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86 Summary:\n",
            "  Train Loss: 0.0515 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.3262 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 87/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87 Summary:\n",
            "  Train Loss: 0.0576 | Train Accuracy: 0.9800\n",
            "  Valid Loss: 0.3346 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 88/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.02it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88 Summary:\n",
            "  Train Loss: 0.0655 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.3297 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 89/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.71it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89 Summary:\n",
            "  Train Loss: 0.0568 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3282 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 90/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.42it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90 Summary:\n",
            "  Train Loss: 0.0603 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.3380 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 91/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.87it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91 Summary:\n",
            "  Train Loss: 0.0550 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3395 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 92/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.21it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92 Summary:\n",
            "  Train Loss: 0.0585 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.3333 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 93/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.51it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93 Summary:\n",
            "  Train Loss: 0.0474 | Train Accuracy: 0.9850\n",
            "  Valid Loss: 0.3321 | Valid Accuracy: 0.8875\n",
            "\n",
            "--- Epoch 94/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.64it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94 Summary:\n",
            "  Train Loss: 0.0523 | Train Accuracy: 0.9869\n",
            "  Valid Loss: 0.3224 | Valid Accuracy: 0.8925\n",
            "✅ New best model saved with validation accuracy: 0.8925\n",
            "\n",
            "--- Epoch 95/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.84it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95 Summary:\n",
            "  Train Loss: 0.0562 | Train Accuracy: 0.9800\n",
            "  Valid Loss: 0.3386 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 96/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.22it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96 Summary:\n",
            "  Train Loss: 0.0572 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3401 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 97/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.44it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97 Summary:\n",
            "  Train Loss: 0.0600 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.3405 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 98/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.51it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98 Summary:\n",
            "  Train Loss: 0.0574 | Train Accuracy: 0.9800\n",
            "  Valid Loss: 0.3312 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 99/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.79it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  9.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99 Summary:\n",
            "  Train Loss: 0.0593 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.3315 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 100/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.17it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100 Summary:\n",
            "  Train Loss: 0.0568 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.3315 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation accuracy achieved: 0.8925\n",
            "Best model saved to 'best_model_v2.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 PREDICTION"
      ],
      "metadata": {
        "id": "9aU23576Iqg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model_v2.pth' # <-- THE ONLY CHANGE NEEDED\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction_v2.json' # Saving to a new file\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture to match the one we trained\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # IMPORTANT: Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY8nQ3_HviCQ",
        "outputId": "2d4e0477-13aa-46c1-d72b-e2a5895eec76"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model from 'best_model_v2.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:00<00:00, 24.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction_v2.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRATIFIED RANDOM SPLITTING /train_epoch_split.py script"
      ],
      "metadata": {
        "id": "x3qRyKUOhSS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model_v4_epoch_split.pth'\n",
        "NUM_EPOCHS = 30 # Let's run for 30 epochs\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Custom Dataset Definition (Same as before) ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms (Same as v2) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 3. Load Master DataFrame (ONCE) ---\n",
        "try:\n",
        "    df_master = pd.read_csv(MASTER_CSV_PATH)\n",
        "    print(f\"Loaded master dataset with {len(df_master)} samples.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found. Please run the data preparation script first.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 4. Model Definition (Same as v2) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Using our best 0.5 dropout\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 5. Loss Function, Optimizer (Same as v2) ---\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # With weight decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 6. Training Loop (with Per-Epoch Splitting) ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- THIS IS YOUR STRATEGY ---\n",
        "    # Re-split the data at the start of every epoch\n",
        "    print(f\"Creating new stratified 80/20 split for Epoch {epoch+1}...\")\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_master,\n",
        "        test_size=0.2,       # 80% training, 20% validation\n",
        "        random_state=epoch,  # Use epoch number as random_state to ensure a NEW split\n",
        "        stratify=df_master['target_label']\n",
        "    )\n",
        "\n",
        "    # Create new Datasets and DataLoaders for this epoch\n",
        "    train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "    val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    # --- END OF NEW LOGIC ---\n",
        "\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    # Note: This is now based on a \"noisy\" val_accuracy\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    # Save the best model\n",
        "    # Note: This is now saving the model that performed best\n",
        "    # on its *specific* 400-image validation set.\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with (noisy) validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Highest validation accuracy achieved on a single epoch split: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SopxdGUeccb9",
        "outputId": "84b9aafc-4317-4a5f-d14d-22cdf69baeb6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded master dataset with 2000 samples.\n",
            "\n",
            "--- Epoch 1/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.34it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5668 | Train Accuracy: 0.7269\n",
            "  Valid Loss: 0.8409 | Valid Accuracy: 0.6400\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.6400\n",
            "\n",
            "--- Epoch 2/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 17.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.4538 | Train Accuracy: 0.8075\n",
            "  Valid Loss: 0.3152 | Valid Accuracy: 0.8725\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.8725\n",
            "\n",
            "--- Epoch 3/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.62it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3525 | Train Accuracy: 0.8444\n",
            "  Valid Loss: 0.3974 | Valid Accuracy: 0.8275\n",
            "\n",
            "--- Epoch 4/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.36it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.3267 | Train Accuracy: 0.8656\n",
            "  Valid Loss: 0.3364 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 5/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2959 | Train Accuracy: 0.8844\n",
            "  Valid Loss: 0.2378 | Valid Accuracy: 0.9075\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9075\n",
            "\n",
            "--- Epoch 6/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.44it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2768 | Train Accuracy: 0.8988\n",
            "  Valid Loss: 0.1898 | Valid Accuracy: 0.9350\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9350\n",
            "\n",
            "--- Epoch 7/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.87it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2545 | Train Accuracy: 0.9125\n",
            "  Valid Loss: 0.2628 | Valid Accuracy: 0.8950\n",
            "\n",
            "--- Epoch 8/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.35it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.2750 | Train Accuracy: 0.8962\n",
            "  Valid Loss: 0.2942 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 9/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.35it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.2500 | Train Accuracy: 0.9044\n",
            "  Valid Loss: 0.3376 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 10/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 15.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.2438 | Train Accuracy: 0.9044\n",
            "  Valid Loss: 0.1698 | Valid Accuracy: 0.9375\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9375\n",
            "\n",
            "--- Epoch 11/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.30it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.2266 | Train Accuracy: 0.9119\n",
            "  Valid Loss: 0.1877 | Valid Accuracy: 0.9375\n",
            "\n",
            "--- Epoch 12/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.32it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.1981 | Train Accuracy: 0.9256\n",
            "  Valid Loss: 0.1664 | Valid Accuracy: 0.9575\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9575\n",
            "\n",
            "--- Epoch 13/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 20.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.2129 | Train Accuracy: 0.9281\n",
            "  Valid Loss: 0.2314 | Valid Accuracy: 0.9000\n",
            "\n",
            "--- Epoch 14/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  6.80it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.2155 | Train Accuracy: 0.9225\n",
            "  Valid Loss: 0.2414 | Valid Accuracy: 0.9075\n",
            "\n",
            "--- Epoch 15/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.2126 | Train Accuracy: 0.9294\n",
            "  Valid Loss: 0.1688 | Valid Accuracy: 0.9450\n",
            "\n",
            "--- Epoch 16/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.33it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.2046 | Train Accuracy: 0.9231\n",
            "  Valid Loss: 0.1289 | Valid Accuracy: 0.9575\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 17/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.13it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.1688 | Train Accuracy: 0.9444\n",
            "  Valid Loss: 0.0832 | Valid Accuracy: 0.9750\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9750\n",
            "\n",
            "--- Epoch 18/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.03it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.1418 | Train Accuracy: 0.9550\n",
            "  Valid Loss: 0.0767 | Valid Accuracy: 0.9750\n",
            "\n",
            "--- Epoch 19/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.30it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.1186 | Train Accuracy: 0.9569\n",
            "  Valid Loss: 0.0598 | Valid Accuracy: 0.9850\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9850\n",
            "\n",
            "--- Epoch 20/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.25it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.1124 | Train Accuracy: 0.9613\n",
            "  Valid Loss: 0.0570 | Valid Accuracy: 0.9875\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9875\n",
            "\n",
            "--- Epoch 21/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 17.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.1189 | Train Accuracy: 0.9606\n",
            "  Valid Loss: 0.0597 | Valid Accuracy: 0.9800\n",
            "\n",
            "--- Epoch 22/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.40it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0951 | Train Accuracy: 0.9644\n",
            "  Valid Loss: 0.0408 | Valid Accuracy: 0.9900\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9900\n",
            "\n",
            "--- Epoch 23/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 20.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0778 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.0550 | Valid Accuracy: 0.9825\n",
            "\n",
            "--- Epoch 24/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.37it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.1049 | Train Accuracy: 0.9625\n",
            "  Valid Loss: 0.0298 | Valid Accuracy: 0.9925\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9925\n",
            "\n",
            "--- Epoch 25/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0971 | Train Accuracy: 0.9650\n",
            "  Valid Loss: 0.0224 | Valid Accuracy: 0.9925\n",
            "\n",
            "--- Epoch 26/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 26...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.87it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0854 | Train Accuracy: 0.9675\n",
            "  Valid Loss: 0.0304 | Valid Accuracy: 0.9925\n",
            "\n",
            "--- Epoch 27/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.36it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0692 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.0250 | Valid Accuracy: 0.9925\n",
            "\n",
            "--- Epoch 28/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 28...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.22it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0744 | Train Accuracy: 0.9744\n",
            "  Valid Loss: 0.0270 | Valid Accuracy: 0.9925\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 29/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 29...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  6.90it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0717 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.0168 | Valid Accuracy: 0.9975\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9975\n",
            "\n",
            "--- Epoch 30/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.22it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0706 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.0364 | Valid Accuracy: 0.9900\n",
            "\n",
            "--- Training Complete ---\n",
            "Highest validation accuracy achieved on a single epoch split: 0.9975\n",
            "Best model saved to 'best_model_v4_epoch_split.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFICATION"
      ],
      "metadata": {
        "id": "oiSrTURokY4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "# --- LOAD THE NEW CHAMPION MODEL ---\n",
        "MODEL_PATH = 'best_model_v4_epoch_split.pth'\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "print(f\"Loading new champion model from '{MODEL_PATH}'...\")\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval() # Set to eval mode\n",
        "\n",
        "# --- 2. Prepare STABLE Validation Data ---\n",
        "# We will use the *exact* same 80/20 split as our original v2 test\n",
        "try:\n",
        "    df = pd.read_csv(MASTER_CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Re-create the *exact* same 80/20 split using random_state=42\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42, # This is our stable, benchmark split\n",
        "    stratify=df['target_label']\n",
        ")\n",
        "print(f\"Loaded our stable validation set of {len(val_df)} images.\")\n",
        "\n",
        "# Create the custom dataset\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_dataset = ValidationDataset(val_df, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- 3. Generate Predictions on the STABLE Validation Set ---\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validating Champion Model\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds_numeric = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        labels_numeric = labels.cpu().numpy()\n",
        "\n",
        "        if preds_numeric.ndim == 0:\n",
        "            preds_numeric = [preds_numeric.item()]\n",
        "            labels_numeric = [labels_numeric.item()]\n",
        "\n",
        "        all_preds.extend(preds_numeric)\n",
        "        all_labels.extend(labels_numeric)\n",
        "\n",
        "# --- 4. Calculate Final \"True\" Accuracy ---\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "print(\"\\n--- FINAL VALIDATION COMPLETE ---\")\n",
        "print(f\"Original v2 Model (Stable Accuracy): 0.9000\")\n",
        "print(f\"New v4 Model (Stable Accuracy)   : {final_accuracy:.4f}\")\n",
        "\n",
        "if final_accuracy > 0.9000:\n",
        "    print(\"\\n✅✅✅ IT'S CONFIRMED! Your strategy worked.\")\n",
        "    print(\"The new model is officially better.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ The original v2 model remains the champion.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCgSRbyNccZI",
        "outputId": "3eeee770-96a5-43bb-f243-a22967df0ceb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading new champion model from 'best_model_v4_epoch_split.pth'...\n",
            "Loaded our stable validation set of 400 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating Champion Model: 100%|██████████| 7/7 [00:00<00:00, 21.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FINAL VALIDATION COMPLETE ---\n",
            "Original v2 Model (Stable Accuracy): 0.9000\n",
            "New v4 Model (Stable Accuracy)   : 0.9925\n",
            "\n",
            "✅✅✅ IT'S CONFIRMED! Your strategy worked.\n",
            "The new model is officially better.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict_final.py script"
      ],
      "metadata": {
        "id": "IL-SaIHdkbN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "# --- LOAD THE CHAMPION MODEL ---\n",
        "MODEL_PATH = 'best_model_v4_epoch_split.pth'\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction_FINAL.json'\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading final champion model from '{MODEL_PATH}'...\")\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, indices in tqdm(test_loader, desc=\"Generating Final Predictions\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        if preds.ndim == 0:\n",
        "            preds = [preds.item()]\n",
        "            indices = [indices.item()]\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Final submission saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77J7Z6OeRxn",
        "outputId": "56144316-f0e3-4c34-b70b-094207f55d6c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading final champion model from 'best_model_v4_epoch_split.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Final Predictions: 100%|██████████| 8/8 [00:00<00:00, 23.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Final submission saved to 'teamname_prediction_FINAL.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlKbFRDWdt5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sj25IwICh7An"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}