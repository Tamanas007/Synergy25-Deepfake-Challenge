{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "unzipping"
      ],
      "metadata": {
        "id": "hQLQt6ftrMft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkq4iTtOp3nf",
        "outputId": "5ec17dcc-2117-4b29-fbe2-a52e79da1602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping '/content/drive/MyDrive/fake_cifake_images.zip'...\n",
            "Successfully unzipped all files to 'hackathon_dataset'\n",
            "\n",
            "Unzipped contents:\n",
            "['fake_cifake_images/995.png', 'fake_cifake_images/996.png', 'fake_cifake_images/957.png', 'fake_cifake_images/1000.png', 'fake_cifake_images/997.png', 'fake_cifake_images/998.png', 'fake_cifake_images/999.png', 'fake_cifake_images/948.png', 'fake_cifake_images/959.png', 'fake_cifake_images/969.png', 'fake_cifake_images/984.png', 'fake_cifake_images/991.png', 'fake_cifake_images/966.png', 'fake_cifake_images/955.png', 'fake_cifake_images/994.png', 'fake_cifake_images/978.png', 'fake_cifake_images/928.png', 'fake_cifake_images/953.png', 'fake_cifake_images/945.png', 'fake_cifake_images/954.png', 'fake_cifake_images/980.png', 'fake_cifake_images/939.png', 'fake_cifake_images/982.png', 'fake_cifake_images/986.png', 'fake_cifake_images/942.png', 'fake_cifake_images/929.png', 'fake_cifake_images/937.png', 'fake_cifake_images/964.png', 'fake_cifake_images/976.png', 'fake_cifake_images/985.png', 'fake_cifake_images/949.png', 'fake_cifake_images/975.png', 'fake_cifake_images/940.png', 'fake_cifake_images/965.png', 'fake_cifake_images/981.png', 'fake_cifake_images/992.png', 'fake_cifake_images/983.png', 'fake_cifake_images/946.png', 'fake_cifake_images/932.png', 'fake_cifake_images/956.png', 'fake_cifake_images/987.png', 'fake_cifake_images/941.png', 'fake_cifake_images/971.png', 'fake_cifake_images/943.png', 'fake_cifake_images/970.png', 'fake_cifake_images/967.png', 'fake_cifake_images/962.png', 'fake_cifake_images/993.png', 'fake_cifake_images/938.png', 'fake_cifake_images/947.png', 'fake_cifake_images/944.png', 'fake_cifake_images/961.png', 'fake_cifake_images/968.png', 'fake_cifake_images/974.png', 'fake_cifake_images/934.png', 'fake_cifake_images/990.png', 'fake_cifake_images/960.png', 'fake_cifake_images/952.png', 'fake_cifake_images/973.png', 'fake_cifake_images/935.png', 'fake_cifake_images/930.png', 'fake_cifake_images/972.png', 'fake_cifake_images/977.png', 'fake_cifake_images/989.png', 'fake_cifake_images/979.png', 'fake_cifake_images/951.png', 'fake_cifake_images/936.png', 'fake_cifake_images/931.png', 'fake_cifake_images/988.png', 'fake_cifake_images/958.png', 'fake_cifake_images/963.png', 'fake_cifake_images/933.png', 'fake_cifake_images/921.png', 'fake_cifake_images/899.png', 'fake_cifake_images/866.png', 'fake_cifake_images/870.png', 'fake_cifake_images/923.png', 'fake_cifake_images/864.png', 'fake_cifake_images/927.png', 'fake_cifake_images/950.png', 'fake_cifake_images/884.png', 'fake_cifake_images/894.png', 'fake_cifake_images/909.png', 'fake_cifake_images/885.png', 'fake_cifake_images/888.png', 'fake_cifake_images/871.png', 'fake_cifake_images/893.png', 'fake_cifake_images/869.png', 'fake_cifake_images/924.png', 'fake_cifake_images/910.png', 'fake_cifake_images/900.png', 'fake_cifake_images/922.png', 'fake_cifake_images/875.png', 'fake_cifake_images/862.png', 'fake_cifake_images/907.png', 'fake_cifake_images/876.png', 'fake_cifake_images/865.png', 'fake_cifake_images/879.png', 'fake_cifake_images/883.png', 'fake_cifake_images/898.png', 'fake_cifake_images/881.png', 'fake_cifake_images/892.png', 'fake_cifake_images/916.png', 'fake_cifake_images/897.png', 'fake_cifake_images/877.png', 'fake_cifake_images/867.png', 'fake_cifake_images/903.png', 'fake_cifake_images/911.png', 'fake_cifake_images/912.png', 'fake_cifake_images/904.png', 'fake_cifake_images/889.png', 'fake_cifake_images/896.png', 'fake_cifake_images/918.png', 'fake_cifake_images/891.png', 'fake_cifake_images/863.png', 'fake_cifake_images/878.png', 'fake_cifake_images/874.png', 'fake_cifake_images/920.png', 'fake_cifake_images/913.png', 'fake_cifake_images/914.png', 'fake_cifake_images/926.png', 'fake_cifake_images/873.png', 'fake_cifake_images/890.png', 'fake_cifake_images/887.png', 'fake_cifake_images/880.png', 'fake_cifake_images/895.png', 'fake_cifake_images/901.png', 'fake_cifake_images/905.png', 'fake_cifake_images/906.png', 'fake_cifake_images/902.png', 'fake_cifake_images/886.png', 'fake_cifake_images/868.png', 'fake_cifake_images/915.png', 'fake_cifake_images/925.png', 'fake_cifake_images/872.png', 'fake_cifake_images/908.png', 'fake_cifake_images/882.png', 'fake_cifake_images/861.png', 'fake_cifake_images/821.png', 'fake_cifake_images/839.png', 'fake_cifake_images/819.png', 'fake_cifake_images/919.png', 'fake_cifake_images/851.png', 'fake_cifake_images/858.png', 'fake_cifake_images/825.png', 'fake_cifake_images/850.png', 'fake_cifake_images/917.png', 'fake_cifake_images/830.png', 'fake_cifake_images/829.png', 'fake_cifake_images/827.png', 'fake_cifake_images/838.png', 'fake_cifake_images/806.png', 'fake_cifake_images/852.png', 'fake_cifake_images/824.png', 'fake_cifake_images/814.png', 'fake_cifake_images/820.png', 'fake_cifake_images/847.png', 'fake_cifake_images/813.png', 'fake_cifake_images/837.png', 'fake_cifake_images/853.png', 'fake_cifake_images/849.png', 'fake_cifake_images/808.png', 'fake_cifake_images/818.png', 'fake_cifake_images/812.png', 'fake_cifake_images/815.png', 'fake_cifake_images/833.png', 'fake_cifake_images/832.png', 'fake_cifake_images/822.png', 'fake_cifake_images/844.png', 'fake_cifake_images/846.png', 'fake_cifake_images/855.png', 'fake_cifake_images/841.png', 'fake_cifake_images/826.png', 'fake_cifake_images/817.png', 'fake_cifake_images/823.png', 'fake_cifake_images/810.png', 'fake_cifake_images/854.png', 'fake_cifake_images/848.png', 'fake_cifake_images/831.png', 'fake_cifake_images/856.png', 'fake_cifake_images/845.png', 'fake_cifake_images/809.png', 'fake_cifake_images/835.png', 'fake_cifake_images/828.png', 'fake_cifake_images/816.png', 'fake_cifake_images/857.png', 'fake_cifake_images/860.png', 'fake_cifake_images/836.png', 'fake_cifake_images/840.png', 'fake_cifake_images/843.png', 'fake_cifake_images/859.png', 'fake_cifake_images/807.png', 'fake_cifake_images/791.png', 'fake_cifake_images/780.png', 'fake_cifake_images/743.png', 'fake_cifake_images/804.png', 'fake_cifake_images/788.png', 'fake_cifake_images/763.png', 'fake_cifake_images/756.png', 'fake_cifake_images/755.png', 'fake_cifake_images/776.png', 'fake_cifake_images/745.png', 'fake_cifake_images/761.png', 'fake_cifake_images/811.png', 'fake_cifake_images/766.png', 'fake_cifake_images/842.png', 'fake_cifake_images/784.png', 'fake_cifake_images/753.png', 'fake_cifake_images/760.png', 'fake_cifake_images/744.png', 'fake_cifake_images/762.png', 'fake_cifake_images/834.png', 'fake_cifake_images/800.png', 'fake_cifake_images/754.png', 'fake_cifake_images/772.png', 'fake_cifake_images/746.png', 'fake_cifake_images/778.png', 'fake_cifake_images/774.png', 'fake_cifake_images/797.png', 'fake_cifake_images/748.png', 'fake_cifake_images/796.png', 'fake_cifake_images/793.png', 'fake_cifake_images/750.png', 'fake_cifake_images/785.png', 'fake_cifake_images/790.png', 'fake_cifake_images/767.png', 'fake_cifake_images/742.png', 'fake_cifake_images/752.png', 'fake_cifake_images/799.png', 'fake_cifake_images/747.png', 'fake_cifake_images/759.png', 'fake_cifake_images/792.png', 'fake_cifake_images/758.png', 'fake_cifake_images/740.png', 'fake_cifake_images/781.png', 'fake_cifake_images/803.png', 'fake_cifake_images/749.png', 'fake_cifake_images/786.png', 'fake_cifake_images/789.png', 'fake_cifake_images/795.png', 'fake_cifake_images/779.png', 'fake_cifake_images/764.png', 'fake_cifake_images/782.png', 'fake_cifake_images/771.png', 'fake_cifake_images/802.png', 'fake_cifake_images/787.png', 'fake_cifake_images/741.png', 'fake_cifake_images/777.png', 'fake_cifake_images/773.png', 'fake_cifake_images/770.png', 'fake_cifake_images/798.png', 'fake_cifake_images/775.png', 'fake_cifake_images/783.png', 'fake_cifake_images/769.png', 'fake_cifake_images/794.png', 'fake_cifake_images/695.png', 'fake_cifake_images/801.png', 'fake_cifake_images/716.png', 'fake_cifake_images/732.png', 'fake_cifake_images/751.png', 'fake_cifake_images/765.png', 'fake_cifake_images/757.png', 'fake_cifake_images/739.png', 'fake_cifake_images/715.png', 'fake_cifake_images/676.png', 'fake_cifake_images/735.png', 'fake_cifake_images/714.png', 'fake_cifake_images/707.png', 'fake_cifake_images/768.png', 'fake_cifake_images/694.png', 'fake_cifake_images/688.png', 'fake_cifake_images/805.png', 'fake_cifake_images/710.png', 'fake_cifake_images/718.png', 'fake_cifake_images/697.png', 'fake_cifake_images/721.png', 'fake_cifake_images/683.png', 'fake_cifake_images/690.png', 'fake_cifake_images/705.png', 'fake_cifake_images/708.png', 'fake_cifake_images/709.png', 'fake_cifake_images/719.png', 'fake_cifake_images/728.png', 'fake_cifake_images/682.png', 'fake_cifake_images/689.png', 'fake_cifake_images/700.png', 'fake_cifake_images/674.png', 'fake_cifake_images/702.png', 'fake_cifake_images/722.png', 'fake_cifake_images/698.png', 'fake_cifake_images/686.png', 'fake_cifake_images/717.png', 'fake_cifake_images/736.png', 'fake_cifake_images/701.png', 'fake_cifake_images/692.png', 'fake_cifake_images/712.png', 'fake_cifake_images/711.png', 'fake_cifake_images/729.png', 'fake_cifake_images/696.png', 'fake_cifake_images/675.png', 'fake_cifake_images/685.png', 'fake_cifake_images/703.png', 'fake_cifake_images/687.png', 'fake_cifake_images/713.png', 'fake_cifake_images/723.png', 'fake_cifake_images/691.png', 'fake_cifake_images/677.png', 'fake_cifake_images/725.png', 'fake_cifake_images/724.png', 'fake_cifake_images/737.png', 'fake_cifake_images/699.png', 'fake_cifake_images/706.png', 'fake_cifake_images/679.png', 'fake_cifake_images/720.png', 'fake_cifake_images/726.png', 'fake_cifake_images/678.png', 'fake_cifake_images/704.png', 'fake_cifake_images/730.png', 'fake_cifake_images/731.png', 'fake_cifake_images/738.png', 'fake_cifake_images/681.png', 'fake_cifake_images/727.png', 'fake_cifake_images/680.png', 'fake_cifake_images/734.png', 'fake_cifake_images/661.png', 'fake_cifake_images/646.png', 'fake_cifake_images/693.png', 'fake_cifake_images/659.png', 'fake_cifake_images/657.png', 'fake_cifake_images/733.png', 'fake_cifake_images/652.png', 'fake_cifake_images/615.png', 'fake_cifake_images/669.png', 'fake_cifake_images/632.png', 'fake_cifake_images/663.png', 'fake_cifake_images/642.png', 'fake_cifake_images/666.png', 'fake_cifake_images/610.png', 'fake_cifake_images/684.png', 'fake_cifake_images/617.png', 'fake_cifake_images/620.png', 'fake_cifake_images/638.png', 'fake_cifake_images/618.png', 'fake_cifake_images/651.png', 'fake_cifake_images/662.png', 'fake_cifake_images/648.png', 'fake_cifake_images/613.png', 'fake_cifake_images/629.png', 'fake_cifake_images/627.png', 'fake_cifake_images/614.png', 'fake_cifake_images/665.png', 'fake_cifake_images/645.png', 'fake_cifake_images/667.png', 'fake_cifake_images/634.png', 'fake_cifake_images/671.png', 'fake_cifake_images/625.png', 'fake_cifake_images/633.png', 'fake_cifake_images/655.png', 'fake_cifake_images/650.png', 'fake_cifake_images/636.png', 'fake_cifake_images/624.png', 'fake_cifake_images/639.png', 'fake_cifake_images/622.png', 'fake_cifake_images/609.png', 'fake_cifake_images/673.png', 'fake_cifake_images/621.png', 'fake_cifake_images/644.png', 'fake_cifake_images/643.png', 'fake_cifake_images/616.png', 'fake_cifake_images/660.png', 'fake_cifake_images/640.png', 'fake_cifake_images/637.png', 'fake_cifake_images/670.png', 'fake_cifake_images/664.png', 'fake_cifake_images/653.png', 'fake_cifake_images/668.png', 'fake_cifake_images/630.png', 'fake_cifake_images/654.png', 'fake_cifake_images/658.png', 'fake_cifake_images/631.png', 'fake_cifake_images/649.png', 'fake_cifake_images/672.png', 'fake_cifake_images/612.png', 'fake_cifake_images/656.png', 'fake_cifake_images/619.png', 'fake_cifake_images/641.png', 'fake_cifake_images/611.png', 'fake_cifake_images/635.png', 'fake_cifake_images/647.png', 'fake_cifake_images/626.png', 'fake_cifake_images/592.png', 'fake_cifake_images/599.png', 'fake_cifake_images/605.png', 'fake_cifake_images/581.png', 'fake_cifake_images/623.png', 'fake_cifake_images/552.png', 'fake_cifake_images/578.png', 'fake_cifake_images/628.png', 'fake_cifake_images/565.png', 'fake_cifake_images/608.png', 'fake_cifake_images/602.png', 'fake_cifake_images/575.png', 'fake_cifake_images/594.png', 'fake_cifake_images/576.png', 'fake_cifake_images/564.png', 'fake_cifake_images/560.png', 'fake_cifake_images/566.png', 'fake_cifake_images/583.png', 'fake_cifake_images/587.png', 'fake_cifake_images/597.png', 'fake_cifake_images/562.png', 'fake_cifake_images/590.png', 'fake_cifake_images/580.png', 'fake_cifake_images/574.png', 'fake_cifake_images/554.png', 'fake_cifake_images/573.png', 'fake_cifake_images/584.png', 'fake_cifake_images/607.png', 'fake_cifake_images/577.png', 'fake_cifake_images/586.png', 'fake_cifake_images/596.png', 'fake_cifake_images/585.png', 'fake_cifake_images/593.png', 'fake_cifake_images/568.png', 'fake_cifake_images/601.png', 'fake_cifake_images/589.png', 'fake_cifake_images/595.png', 'fake_cifake_images/571.png', 'fake_cifake_images/561.png', 'fake_cifake_images/555.png', 'fake_cifake_images/591.png', 'fake_cifake_images/600.png', 'fake_cifake_images/553.png', 'fake_cifake_images/588.png', 'fake_cifake_images/567.png', 'fake_cifake_images/569.png', 'fake_cifake_images/603.png', 'fake_cifake_images/582.png', 'fake_cifake_images/558.png', 'fake_cifake_images/556.png', 'fake_cifake_images/570.png', 'fake_cifake_images/559.png', 'fake_cifake_images/604.png', 'fake_cifake_images/579.png', 'fake_cifake_images/551.png', 'fake_cifake_images/563.png', 'fake_cifake_images/606.png', 'fake_cifake_images/500.png', 'fake_cifake_images/598.png', 'fake_cifake_images/539.png', 'fake_cifake_images/557.png', 'fake_cifake_images/540.png', 'fake_cifake_images/524.png', 'fake_cifake_images/511.png', 'fake_cifake_images/538.png', 'fake_cifake_images/572.png', 'fake_cifake_images/515.png', 'fake_cifake_images/549.png', 'fake_cifake_images/498.png', 'fake_cifake_images/522.png', 'fake_cifake_images/493.png', 'fake_cifake_images/545.png', 'fake_cifake_images/543.png', 'fake_cifake_images/530.png', 'fake_cifake_images/517.png', 'fake_cifake_images/491.png', 'fake_cifake_images/507.png', 'fake_cifake_images/537.png', 'fake_cifake_images/520.png', 'fake_cifake_images/492.png', 'fake_cifake_images/526.png', 'fake_cifake_images/487.png', 'fake_cifake_images/519.png', 'fake_cifake_images/497.png', 'fake_cifake_images/532.png', 'fake_cifake_images/548.png', 'fake_cifake_images/534.png', 'fake_cifake_images/495.png', 'fake_cifake_images/505.png', 'fake_cifake_images/536.png', 'fake_cifake_images/550.png', 'fake_cifake_images/527.png', 'fake_cifake_images/531.png', 'fake_cifake_images/518.png', 'fake_cifake_images/506.png', 'fake_cifake_images/512.png', 'fake_cifake_images/501.png', 'fake_cifake_images/533.png', 'fake_cifake_images/521.png', 'fake_cifake_images/510.png', 'fake_cifake_images/489.png', 'fake_cifake_images/544.png', 'fake_cifake_images/504.png', 'fake_cifake_images/528.png', 'fake_cifake_images/523.png', 'fake_cifake_images/516.png', 'fake_cifake_images/494.png', 'fake_cifake_images/541.png', 'fake_cifake_images/509.png', 'fake_cifake_images/513.png', 'fake_cifake_images/503.png', 'fake_cifake_images/529.png', 'fake_cifake_images/502.png', 'fake_cifake_images/546.png', 'fake_cifake_images/488.png', 'fake_cifake_images/542.png', 'fake_cifake_images/472.png', 'fake_cifake_images/514.png', 'fake_cifake_images/547.png', 'fake_cifake_images/499.png', 'fake_cifake_images/431.png', 'fake_cifake_images/496.png', 'fake_cifake_images/490.png', 'fake_cifake_images/508.png', 'fake_cifake_images/432.png', 'fake_cifake_images/525.png', 'fake_cifake_images/449.png', 'fake_cifake_images/476.png', 'fake_cifake_images/480.png', 'fake_cifake_images/474.png', 'fake_cifake_images/446.png', 'fake_cifake_images/535.png', 'fake_cifake_images/448.png', 'fake_cifake_images/434.png', 'fake_cifake_images/467.png', 'fake_cifake_images/471.png', 'fake_cifake_images/430.png', 'fake_cifake_images/440.png', 'fake_cifake_images/482.png', 'fake_cifake_images/462.png', 'fake_cifake_images/468.png', 'fake_cifake_images/451.png', 'fake_cifake_images/456.png', 'fake_cifake_images/461.png', 'fake_cifake_images/424.png', 'fake_cifake_images/465.png', 'fake_cifake_images/447.png', 'fake_cifake_images/443.png', 'fake_cifake_images/481.png', 'fake_cifake_images/454.png', 'fake_cifake_images/423.png', 'fake_cifake_images/469.png', 'fake_cifake_images/437.png', 'fake_cifake_images/478.png', 'fake_cifake_images/433.png', 'fake_cifake_images/470.png', 'fake_cifake_images/484.png', 'fake_cifake_images/444.png', 'fake_cifake_images/436.png', 'fake_cifake_images/477.png', 'fake_cifake_images/439.png', 'fake_cifake_images/450.png', 'fake_cifake_images/428.png', 'fake_cifake_images/455.png', 'fake_cifake_images/458.png', 'fake_cifake_images/463.png', 'fake_cifake_images/429.png', 'fake_cifake_images/473.png', 'fake_cifake_images/466.png', 'fake_cifake_images/479.png', 'fake_cifake_images/438.png', 'fake_cifake_images/442.png', 'fake_cifake_images/426.png', 'fake_cifake_images/464.png', 'fake_cifake_images/460.png', 'fake_cifake_images/483.png', 'fake_cifake_images/457.png', 'fake_cifake_images/485.png', 'fake_cifake_images/425.png', 'fake_cifake_images/486.png', 'fake_cifake_images/453.png', 'fake_cifake_images/459.png', 'fake_cifake_images/441.png', 'fake_cifake_images/435.png', 'fake_cifake_images/475.png', 'fake_cifake_images/427.png', 'fake_cifake_images/416.png', 'fake_cifake_images/372.png', 'fake_cifake_images/390.png', 'fake_cifake_images/364.png', 'fake_cifake_images/400.png', 'fake_cifake_images/379.png', 'fake_cifake_images/445.png', 'fake_cifake_images/385.png', 'fake_cifake_images/365.png', 'fake_cifake_images/452.png', 'fake_cifake_images/361.png', 'fake_cifake_images/357.png', 'fake_cifake_images/412.png', 'fake_cifake_images/384.png', 'fake_cifake_images/377.png', 'fake_cifake_images/362.png', 'fake_cifake_images/410.png', 'fake_cifake_images/383.png', 'fake_cifake_images/421.png', 'fake_cifake_images/368.png', 'fake_cifake_images/375.png', 'fake_cifake_images/420.png', 'fake_cifake_images/407.png', 'fake_cifake_images/369.png', 'fake_cifake_images/380.png', 'fake_cifake_images/401.png', 'fake_cifake_images/402.png', 'fake_cifake_images/418.png', 'fake_cifake_images/403.png', 'fake_cifake_images/363.png', 'fake_cifake_images/378.png', 'fake_cifake_images/392.png', 'fake_cifake_images/371.png', 'fake_cifake_images/398.png', 'fake_cifake_images/373.png', 'fake_cifake_images/388.png', 'fake_cifake_images/374.png', 'fake_cifake_images/382.png', 'fake_cifake_images/414.png', 'fake_cifake_images/404.png', 'fake_cifake_images/386.png', 'fake_cifake_images/399.png', 'fake_cifake_images/397.png', 'fake_cifake_images/387.png', 'fake_cifake_images/422.png', 'fake_cifake_images/405.png', 'fake_cifake_images/415.png', 'fake_cifake_images/395.png', 'fake_cifake_images/389.png', 'fake_cifake_images/406.png', 'fake_cifake_images/394.png', 'fake_cifake_images/413.png', 'fake_cifake_images/417.png', 'fake_cifake_images/360.png', 'fake_cifake_images/367.png', 'fake_cifake_images/366.png', 'fake_cifake_images/381.png', 'fake_cifake_images/409.png', 'fake_cifake_images/359.png', 'fake_cifake_images/411.png', 'fake_cifake_images/408.png', 'fake_cifake_images/356.png', 'fake_cifake_images/376.png', 'fake_cifake_images/419.png', 'fake_cifake_images/396.png', 'fake_cifake_images/358.png', 'fake_cifake_images/354.png', 'fake_cifake_images/324.png', 'fake_cifake_images/370.png', 'fake_cifake_images/341.png', 'fake_cifake_images/391.png', 'fake_cifake_images/336.png', 'fake_cifake_images/393.png', 'fake_cifake_images/327.png', 'fake_cifake_images/301.png', 'fake_cifake_images/340.png', 'fake_cifake_images/331.png', 'fake_cifake_images/332.png', 'fake_cifake_images/303.png', 'fake_cifake_images/349.png', 'fake_cifake_images/320.png', 'fake_cifake_images/314.png', 'fake_cifake_images/353.png', 'fake_cifake_images/334.png', 'fake_cifake_images/346.png', 'fake_cifake_images/306.png', 'fake_cifake_images/326.png', 'fake_cifake_images/330.png', 'fake_cifake_images/305.png', 'fake_cifake_images/352.png', 'fake_cifake_images/321.png', 'fake_cifake_images/299.png', 'fake_cifake_images/347.png', 'fake_cifake_images/342.png', 'fake_cifake_images/335.png', 'fake_cifake_images/350.png', 'fake_cifake_images/345.png', 'fake_cifake_images/300.png', 'fake_cifake_images/307.png', 'fake_cifake_images/309.png', 'fake_cifake_images/302.png', 'fake_cifake_images/333.png', 'fake_cifake_images/339.png', 'fake_cifake_images/325.png', 'fake_cifake_images/351.png', 'fake_cifake_images/343.png', 'fake_cifake_images/308.png', 'fake_cifake_images/348.png', 'fake_cifake_images/315.png', 'fake_cifake_images/344.png', 'fake_cifake_images/310.png', 'fake_cifake_images/337.png', 'fake_cifake_images/304.png', 'fake_cifake_images/322.png', 'fake_cifake_images/313.png', 'fake_cifake_images/318.png', 'fake_cifake_images/316.png', 'fake_cifake_images/329.png', 'fake_cifake_images/317.png', 'fake_cifake_images/319.png', 'fake_cifake_images/312.png', 'fake_cifake_images/286.png', 'fake_cifake_images/355.png', 'fake_cifake_images/311.png', 'fake_cifake_images/328.png', 'fake_cifake_images/338.png', 'fake_cifake_images/262.png', 'fake_cifake_images/265.png', 'fake_cifake_images/247.png', 'fake_cifake_images/254.png', 'fake_cifake_images/250.png', 'fake_cifake_images/252.png', 'fake_cifake_images/235.png', 'fake_cifake_images/323.png', 'fake_cifake_images/295.png', 'fake_cifake_images/296.png', 'fake_cifake_images/277.png', 'fake_cifake_images/256.png', 'fake_cifake_images/285.png', 'fake_cifake_images/242.png', 'fake_cifake_images/255.png', 'fake_cifake_images/297.png', 'fake_cifake_images/237.png', 'fake_cifake_images/288.png', 'fake_cifake_images/258.png', 'fake_cifake_images/257.png', 'fake_cifake_images/273.png', 'fake_cifake_images/267.png', 'fake_cifake_images/232.png', 'fake_cifake_images/240.png', 'fake_cifake_images/274.png', 'fake_cifake_images/294.png', 'fake_cifake_images/293.png', 'fake_cifake_images/246.png', 'fake_cifake_images/275.png', 'fake_cifake_images/289.png', 'fake_cifake_images/239.png', 'fake_cifake_images/261.png', 'fake_cifake_images/253.png', 'fake_cifake_images/268.png', 'fake_cifake_images/266.png', 'fake_cifake_images/298.png', 'fake_cifake_images/283.png', 'fake_cifake_images/287.png', 'fake_cifake_images/264.png', 'fake_cifake_images/236.png', 'fake_cifake_images/245.png', 'fake_cifake_images/290.png', 'fake_cifake_images/291.png', 'fake_cifake_images/292.png', 'fake_cifake_images/259.png', 'fake_cifake_images/234.png', 'fake_cifake_images/270.png', 'fake_cifake_images/251.png', 'fake_cifake_images/269.png', 'fake_cifake_images/272.png', 'fake_cifake_images/278.png', 'fake_cifake_images/279.png', 'fake_cifake_images/276.png', 'fake_cifake_images/243.png', 'fake_cifake_images/249.png', 'fake_cifake_images/238.png', 'fake_cifake_images/271.png', 'fake_cifake_images/233.png', 'fake_cifake_images/281.png', 'fake_cifake_images/263.png', 'fake_cifake_images/280.png', 'fake_cifake_images/248.png', 'fake_cifake_images/244.png', 'fake_cifake_images/168.png', 'fake_cifake_images/282.png', 'fake_cifake_images/171.png', 'fake_cifake_images/241.png', 'fake_cifake_images/215.png', 'fake_cifake_images/260.png', 'fake_cifake_images/225.png', 'fake_cifake_images/284.png', 'fake_cifake_images/209.png', 'fake_cifake_images/201.png', 'fake_cifake_images/224.png', 'fake_cifake_images/183.png', 'fake_cifake_images/223.png', 'fake_cifake_images/182.png', 'fake_cifake_images/214.png', 'fake_cifake_images/195.png', 'fake_cifake_images/230.png', 'fake_cifake_images/231.png', 'fake_cifake_images/228.png', 'fake_cifake_images/199.png', 'fake_cifake_images/189.png', 'fake_cifake_images/169.png', 'fake_cifake_images/185.png', 'fake_cifake_images/212.png', 'fake_cifake_images/191.png', 'fake_cifake_images/203.png', 'fake_cifake_images/184.png', 'fake_cifake_images/180.png', 'fake_cifake_images/190.png', 'fake_cifake_images/179.png', 'fake_cifake_images/174.png', 'fake_cifake_images/206.png', 'fake_cifake_images/208.png', 'fake_cifake_images/205.png', 'fake_cifake_images/207.png', 'fake_cifake_images/220.png', 'fake_cifake_images/176.png', 'fake_cifake_images/192.png', 'fake_cifake_images/170.png', 'fake_cifake_images/196.png', 'fake_cifake_images/217.png', 'fake_cifake_images/210.png', 'fake_cifake_images/167.png', 'fake_cifake_images/219.png', 'fake_cifake_images/186.png', 'fake_cifake_images/181.png', 'fake_cifake_images/175.png', 'fake_cifake_images/218.png', 'fake_cifake_images/229.png', 'fake_cifake_images/178.png', 'fake_cifake_images/202.png', 'fake_cifake_images/177.png', 'fake_cifake_images/194.png', 'fake_cifake_images/173.png', 'fake_cifake_images/200.png', 'fake_cifake_images/188.png', 'fake_cifake_images/213.png', 'fake_cifake_images/222.png', 'fake_cifake_images/221.png', 'fake_cifake_images/227.png', 'fake_cifake_images/226.png', 'fake_cifake_images/198.png', 'fake_cifake_images/211.png', 'fake_cifake_images/131.png', 'fake_cifake_images/204.png', 'fake_cifake_images/187.png', 'fake_cifake_images/193.png', 'fake_cifake_images/197.png', 'fake_cifake_images/112.png', 'fake_cifake_images/120.png', 'fake_cifake_images/118.png', 'fake_cifake_images/139.png', 'fake_cifake_images/216.png', 'fake_cifake_images/143.png', 'fake_cifake_images/145.png', 'fake_cifake_images/148.png', 'fake_cifake_images/125.png', 'fake_cifake_images/127.png', 'fake_cifake_images/123.png', 'fake_cifake_images/142.png', 'fake_cifake_images/136.png', 'fake_cifake_images/157.png', 'fake_cifake_images/153.png', 'fake_cifake_images/129.png', 'fake_cifake_images/172.png', 'fake_cifake_images/114.png', 'fake_cifake_images/130.png', 'fake_cifake_images/117.png', 'fake_cifake_images/133.png', 'fake_cifake_images/144.png', 'fake_cifake_images/115.png', 'fake_cifake_images/113.png', 'fake_cifake_images/155.png', 'fake_cifake_images/121.png', 'fake_cifake_images/122.png', 'fake_cifake_images/151.png', 'fake_cifake_images/166.png', 'fake_cifake_images/134.png', 'fake_cifake_images/146.png', 'fake_cifake_images/124.png', 'fake_cifake_images/138.png', 'fake_cifake_images/159.png', 'fake_cifake_images/141.png', 'fake_cifake_images/135.png', 'fake_cifake_images/162.png', 'fake_cifake_images/119.png', 'fake_cifake_images/160.png', 'fake_cifake_images/164.png', 'fake_cifake_images/128.png', 'fake_cifake_images/109.png', 'fake_cifake_images/156.png', 'fake_cifake_images/150.png', 'fake_cifake_images/163.png', 'fake_cifake_images/110.png', 'fake_cifake_images/158.png', 'fake_cifake_images/149.png', 'fake_cifake_images/140.png', 'fake_cifake_images/111.png', 'fake_cifake_images/132.png', 'fake_cifake_images/165.png', 'fake_cifake_images/58.png', 'fake_cifake_images/126.png', 'fake_cifake_images/147.png', 'fake_cifake_images/137.png', 'fake_cifake_images/93.png', 'fake_cifake_images/161.png', 'fake_cifake_images/154.png', 'fake_cifake_images/59.png', 'fake_cifake_images/82.png', 'fake_cifake_images/106.png', 'fake_cifake_images/98.png', 'fake_cifake_images/44.png', 'fake_cifake_images/83.png', 'fake_cifake_images/65.png', 'fake_cifake_images/91.png', 'fake_cifake_images/80.png', 'fake_cifake_images/69.png', 'fake_cifake_images/48.png', 'fake_cifake_images/116.png', 'fake_cifake_images/81.png', 'fake_cifake_images/66.png', 'fake_cifake_images/152.png', 'fake_cifake_images/50.png', 'fake_cifake_images/74.png', 'fake_cifake_images/97.png', 'fake_cifake_images/46.png', 'fake_cifake_images/45.png', 'fake_cifake_images/64.png', 'fake_cifake_images/73.png', 'fake_cifake_images/90.png', 'fake_cifake_images/95.png', 'fake_cifake_images/67.png', 'fake_cifake_images/52.png', 'fake_cifake_images/105.png', 'fake_cifake_images/102.png', 'fake_cifake_images/99.png', 'fake_cifake_images/96.png', 'fake_cifake_images/51.png', 'fake_cifake_images/88.png', 'fake_cifake_images/77.png', 'fake_cifake_images/101.png', 'fake_cifake_images/100.png', 'fake_cifake_images/63.png', 'fake_cifake_images/78.png', 'fake_cifake_images/54.png', 'fake_cifake_images/92.png', 'fake_cifake_images/43.png', 'fake_cifake_images/94.png', 'fake_cifake_images/61.png', 'fake_cifake_images/107.png', 'fake_cifake_images/84.png', 'fake_cifake_images/75.png', 'fake_cifake_images/85.png', 'fake_cifake_images/72.png', 'fake_cifake_images/70.png', 'fake_cifake_images/108.png', 'fake_cifake_images/86.png', 'fake_cifake_images/87.png', 'fake_cifake_images/104.png', 'fake_cifake_images/53.png', 'fake_cifake_images/49.png', 'fake_cifake_images/71.png', 'fake_cifake_images/89.png', 'fake_cifake_images/55.png', 'fake_cifake_images/79.png', 'fake_cifake_images/76.png', 'fake_cifake_images/103.png', 'fake_cifake_images/32.png', 'fake_cifake_images/19.png', 'fake_cifake_images/60.png', 'fake_cifake_images/56.png', 'fake_cifake_images/5.png', 'fake_cifake_images/57.png', 'fake_cifake_images/4.png', 'fake_cifake_images/14.png', 'fake_cifake_images/28.png', 'fake_cifake_images/68.png', 'fake_cifake_images/3.png', 'fake_cifake_images/18.png', 'fake_cifake_images/6.png', 'fake_cifake_images/62.png', 'fake_cifake_images/8.png', 'fake_cifake_images/9.png', 'fake_cifake_images/35.png', 'fake_cifake_images/22.png', 'fake_cifake_images/38.png', 'fake_cifake_images/37.png', 'fake_cifake_images/24.png', 'fake_cifake_images/39.png', 'fake_cifake_images/47.png', 'fake_cifake_images/36.png', 'fake_cifake_images/27.png', 'fake_cifake_images/13.png', 'fake_cifake_images/33.png', 'fake_cifake_images/26.png', 'fake_cifake_images/40.png', 'fake_cifake_images/29.png', 'fake_cifake_images/31.png', 'fake_cifake_images/30.png', 'fake_cifake_images/21.png', 'fake_cifake_images/11.png', 'fake_cifake_images/25.png', 'fake_cifake_images/17.png', 'fake_cifake_images/41.png', 'fake_cifake_images/1.png', 'fake_cifake_images/23.png', 'fake_cifake_images/12.png', 'fake_cifake_images/42.png', 'fake_cifake_images/7.png', 'fake_cifake_images/16.png', 'fake_cifake_images/34.png', 'fake_cifake_images/2.png', 'fake_cifake_images/20.png', 'fake_cifake_images/10.png', 'fake_cifake_images/15.png']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- 1. Set your paths ---\n",
        "\n",
        "# This is the name of the file you downloaded from the hackathon.\n",
        "# (e.g., \"Synergy25_dataset.zip\")\n",
        "zip_file_path = '/content/drive/MyDrive/fake_cifake_images.zip'\n",
        "\n",
        "# This is the name of the folder where you want all the files to go.\n",
        "# (e.t., \"dataset/\")\n",
        "destination_folder = 'hackathon_dataset'\n",
        "\n",
        "# --- 2. Create the destination folder if it doesn't exist ---\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "    print(f\"Created directory: {destination_folder}\")\n",
        "\n",
        "# --- 3. Unzip the file ---\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        print(f\"Unzipping '{zip_file_path}'...\")\n",
        "        zip_ref.extractall(destination_folder)\n",
        "        print(f\"Successfully unzipped all files to '{destination_folder}'\")\n",
        "\n",
        "        # Optional: List the files you unzipped\n",
        "        print(\"\\nUnzipped contents:\")\n",
        "        print(zip_ref.namelist())\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file '{zip_file_path}' is not a valid zip file or is corrupted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{zip_file_path}' was not found.\")\n",
        "    print(\"Please make sure the file is in the same directory as this script, or provide the full path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "0xZekcdFuNKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import collections\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# It should contain the 5 subfolders ('real images', 'fake images', etc.)\n",
        "BASE_DATA_DIR = 'hackathon_dataset'\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/real_cifake_images')\n",
        "FAKE_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/fake_cifake_images')\n",
        "TEST_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/test')\n",
        "REAL_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/real_cifake_preds.json', '/content/drive/MyDrive/real_cifake_preds.json') # Assuming file is named this\n",
        "FAKE_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/fake_cifake_preds.json', '/content/drive/MyDrive/fake_cifake_preds.json') # Assuming file is named this\n",
        "\n",
        "print(\"--- Starting Dataset Verification ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 1: File Count Sanity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 1: File Count Sanity Check]\")\n",
        "try:\n",
        "    num_real_images = len(os.listdir(REAL_IMG_DIR))\n",
        "    num_fake_images = len(os.listdir(FAKE_IMG_DIR))\n",
        "    num_test_images = len(os.listdir(TEST_IMG_DIR))\n",
        "\n",
        "    print(f\"Found {num_real_images} images in 'real images' folder.\")\n",
        "    print(f\"Found {num_fake_images} images in 'fake images' folder.\")\n",
        "    print(f\"Found {num_test_images} images in 'test image' folder.\")\n",
        "\n",
        "    if num_real_images == 1000 and num_fake_images == 1000:\n",
        "        print(\"✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Image counts do not match the expected 1000/1000 split.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A folder was not found. Please check your paths. Details: {e}\")\n",
        "    exit() # Stop the script if basic folders are missing\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 2: The \"Imperfect Model\" Check (JSON Analysis)\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 2: JSON Prediction Analysis]\")\n",
        "try:\n",
        "    with open(REAL_JSON_PATH, 'r') as f:\n",
        "        real_json_data = json.load(f)\n",
        "    with open(FAKE_JSON_PATH, 'r') as f:\n",
        "        fake_json_data = json.load(f)\n",
        "\n",
        "    # Count predictions in the JSON for REAL images\n",
        "    real_json_counts = collections.Counter(item['prediction'] for item in real_json_data)\n",
        "    print(\"Proprietary model's predictions on REAL images:\")\n",
        "    print(f\"  - Predicted 'real': {real_json_counts.get('real', 0)}\")\n",
        "    print(f\"  - Predicted 'fake': {real_json_counts.get('fake', 0)}\")\n",
        "\n",
        "    # Count predictions in the JSON for FAKE images\n",
        "    fake_json_counts = collections.Counter(item['prediction'] for item in fake_json_data)\n",
        "    print(\"Proprietary model's predictions on FAKE images:\")\n",
        "    print(f\"  - Predicted 'fake': {fake_json_counts.get('fake', 0)}\")\n",
        "    print(f\"  - Predicted 'real': {fake_json_counts.get('real', 0)}\")\n",
        "\n",
        "    # --- The CRITICAL VERDICT ---\n",
        "    if real_json_counts.get('fake', 0) == 0 and fake_json_counts.get('real', 0) == 0:\n",
        "        print(\"✅ STATUS: The proprietary model is 'perfect' on the training set.\")\n",
        "        print(\"   Our task is a standard, balanced binary classification.\")\n",
        "    else:\n",
        "        print(\"⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\")\n",
        "        print(\"   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A JSON file was not found. Please check your JSON file names and paths. Details: {e}\")\n",
        "    exit()\n",
        "except json.JSONDecodeError:\n",
        "    print(\"❌ ERROR: Could not parse a JSON file. It might be corrupted.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 3: Image Format & Integrity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\")\n",
        "image_sizes = set()\n",
        "image_modes = set()\n",
        "corrupted_files = []\n",
        "\n",
        "def check_images(directory, num_to_check=10):\n",
        "    files = os.listdir(directory)\n",
        "    for i, filename in enumerate(files):\n",
        "        if i >= num_to_check:\n",
        "            break\n",
        "        try:\n",
        "            with Image.open(os.path.join(directory, filename)) as img:\n",
        "                image_sizes.add(img.size)\n",
        "                image_modes.add(img.mode)\n",
        "        except Exception as e:\n",
        "            corrupted_files.append(os.path.join(directory, filename))\n",
        "\n",
        "try:\n",
        "    check_images(REAL_IMG_DIR)\n",
        "    check_images(FAKE_IMG_DIR)\n",
        "\n",
        "    print(f\"Found image sizes: {image_sizes}\")\n",
        "    print(f\"Found image modes (e.g., RGB, L): {image_modes}\")\n",
        "\n",
        "    if len(image_sizes) == 1:\n",
        "        print(\"✅ STATUS: All tested images have a consistent size.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying sizes. We will need to resize them all.\")\n",
        "\n",
        "    if len(image_modes) == 1 and 'RGB' in image_modes:\n",
        "        print(\"✅ STATUS: All tested images are in consistent 'RGB' mode.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying modes (e.g., Grayscale 'L') or are not RGB.\")\n",
        "\n",
        "    if not corrupted_files:\n",
        "        print(\"✅ STATUS: No corrupted images found in the sample.\")\n",
        "    else:\n",
        "        print(f\"⚠️ WARNING: Found {len(corrupted_files)} corrupted images: {corrupted_files}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: An unexpected error occurred during image check. Details: {e}\")\n",
        "\n",
        "print(\"\\n--- Verification Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJMbRaNqp54Y",
        "outputId": "2f2ec23b-93d2-4a74-a02f-6cc6c0387fe4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Dataset Verification ---\n",
            "\n",
            "[CHECK 1: File Count Sanity Check]\n",
            "Found 1000 images in 'real images' folder.\n",
            "Found 1000 images in 'fake images' folder.\n",
            "Found 500 images in 'test image' folder.\n",
            "✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\n",
            "\n",
            "[CHECK 2: JSON Prediction Analysis]\n",
            "Proprietary model's predictions on REAL images:\n",
            "  - Predicted 'real': 976\n",
            "  - Predicted 'fake': 24\n",
            "Proprietary model's predictions on FAKE images:\n",
            "  - Predicted 'fake': 988\n",
            "  - Predicted 'real': 12\n",
            "⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\n",
            "   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\n",
            "\n",
            "[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\n",
            "Found image sizes: {(32, 32)}\n",
            "Found image modes (e.g., RGB, L): {'RGB'}\n",
            "✅ STATUS: All tested images have a consistent size.\n",
            "✅ STATUS: All tested images are in consistent 'RGB' mode.\n",
            "✅ STATUS: No corrupted images found in the sample.\n",
            "\n",
            "--- Verification Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# BASE_DATA_DIR = 'hackathon_dataset' # No longer needed with absolute paths\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = '/content/hackathon_dataset/real_cifake_images'\n",
        "FAKE_IMG_DIR = '/content/hackathon_dataset/fake_cifake_images'\n",
        "REAL_JSON_PATH = '/content/drive/MyDrive/real_cifake_preds.json'\n",
        "FAKE_JSON_PATH = '/content/drive/MyDrive/fake_cifake_preds.json'\n",
        "\n",
        "# Output file name\n",
        "OUTPUT_CSV_PATH = 'master_labels.csv'\n",
        "\n",
        "def process_data(image_dir, json_path, data_list):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and an image directory, and populates a list with\n",
        "    image paths and their corresponding target labels.\n",
        "    \"\"\"\n",
        "    print(f\"Processing data from: {os.path.basename(json_path)}\")\n",
        "\n",
        "    # --- Load the JSON prediction data ---\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            predictions = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: JSON file not found at {json_path}. Please check the path and filename.\")\n",
        "        return False\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"❌ ERROR: Could not decode JSON from {json_path}. The file might be corrupted.\")\n",
        "        return False\n",
        "\n",
        "    # --- Create a dictionary for quick lookup: {index: prediction} ---\n",
        "    prediction_map = {item['index']: item['prediction'] for item in predictions}\n",
        "\n",
        "    # --- Iterate through images and create the master list ---\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for filename in image_files:\n",
        "        # Assumes image filenames are like \"1.jpg\", \"2.png\", etc.\n",
        "        # We extract the number to use as the index.\n",
        "        try:\n",
        "            # Get the base name without extension (e.g., \"1\") and convert to integer\n",
        "            file_index = int(os.path.splitext(filename)[0])\n",
        "        except ValueError:\n",
        "            print(f\"⚠️ Warning: Could not parse index from filename '{filename}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        if file_index in prediction_map:\n",
        "            prediction_str = prediction_map[file_index]\n",
        "\n",
        "            # Encode labels: \"real\" -> 0, \"fake\" -> 1\n",
        "            target_label = 1 if prediction_str == 'fake' else 0\n",
        "\n",
        "            # Get the full path to the image\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            data_list.append({\n",
        "                'image_path': image_path,\n",
        "                'target_label': target_label\n",
        "            })\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: No prediction found in JSON for image index {file_index} ('{filename}').\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data preparation process.\"\"\"\n",
        "    print(\"--- Starting Step 1: Data Preparation ---\")\n",
        "\n",
        "    master_data_list = []\n",
        "\n",
        "    # Process the \"real\" images and their corresponding JSON predictions\n",
        "    if not process_data(REAL_IMG_DIR, REAL_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # Process the \"fake\" images and their corresponding JSON predictions\n",
        "    if not process_data(FAKE_IMG_DIR, FAKE_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # --- Convert the list to a pandas DataFrame ---\n",
        "    if not master_data_list:\n",
        "        print(\"❌ ERROR: No data was processed. The master list is empty. Halting.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(master_data_list)\n",
        "\n",
        "    # --- Shuffle the DataFrame to mix real and fake samples ---\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # --- Save the final DataFrame to a CSV file ---\n",
        "    try:\n",
        "        df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "        print(f\"\\n✅ Success! Created master dataset with {len(df)} entries.\")\n",
        "        print(f\"   Saved to '{OUTPUT_CSV_PATH}'.\")\n",
        "\n",
        "        # Display the first few rows and the class distribution\n",
        "        print(\"\\n--- Dataset Preview ---\")\n",
        "        print(df.head())\n",
        "        print(\"\\n--- Final Label Distribution ---\")\n",
        "        print(df['target_label'].value_counts())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Could not save the CSV file. Details: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMztruRp52v",
        "outputId": "2acd15ee-2322-4637-b743-8cd49276f383"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 1: Data Preparation ---\n",
            "Processing data from: real_cifake_preds.json\n",
            "Processing data from: fake_cifake_preds.json\n",
            "\n",
            "✅ Success! Created master dataset with 2000 entries.\n",
            "   Saved to 'master_labels.csv'.\n",
            "\n",
            "--- Dataset Preview ---\n",
            "                                          image_path  target_label\n",
            "0  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "1  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "2  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "3  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "4  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "\n",
            "--- Final Label Distribution ---\n",
            "target_label\n",
            "1    1012\n",
            "0     988\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING v1"
      ],
      "metadata": {
        "id": "teUtKSXMIkbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model.pth'\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32 # Based on our verification step\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            # Return a dummy image and label if file is missing\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "try:\n",
        "    df = pd.read_csv(MASTER_CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found. Please run the data preparation script first.\")\n",
        "    exit()\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,       # 80% training, 20% validation\n",
        "    random_state=42,\n",
        "    stratify=df['target_label'] # CRITICAL for maintaining label distribution\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1) # Output is a single value\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss() # Handles the sigmoid activation internally\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Announce LR change manually if it happens\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H_tZWo5p5xk",
        "outputId": "b09df48b-15d2-4bf9-dfa5-058791513655"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 127MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:04<00:00,  5.90it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5449 | Train Accuracy: 0.7369\n",
            "  Valid Loss: 0.7272 | Valid Accuracy: 0.7000\n",
            "✅ New best model saved with validation accuracy: 0.7000\n",
            "\n",
            "--- Epoch 2/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.75it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.3644 | Train Accuracy: 0.8594\n",
            "  Valid Loss: 0.4144 | Valid Accuracy: 0.8200\n",
            "✅ New best model saved with validation accuracy: 0.8200\n",
            "\n",
            "--- Epoch 3/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.30it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 25.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3124 | Train Accuracy: 0.8781\n",
            "  Valid Loss: 0.4805 | Valid Accuracy: 0.8150\n",
            "\n",
            "--- Epoch 4/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 20.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2695 | Train Accuracy: 0.8925\n",
            "  Valid Loss: 0.4720 | Valid Accuracy: 0.8425\n",
            "✅ New best model saved with validation accuracy: 0.8425\n",
            "\n",
            "--- Epoch 5/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.26it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2052 | Train Accuracy: 0.9256\n",
            "  Valid Loss: 0.5468 | Valid Accuracy: 0.8150\n",
            "\n",
            "--- Epoch 6/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.76it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2309 | Train Accuracy: 0.9250\n",
            "  Valid Loss: 0.3803 | Valid Accuracy: 0.8425\n",
            "\n",
            "--- Epoch 7/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.23it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2322 | Train Accuracy: 0.9244\n",
            "  Valid Loss: 0.6239 | Valid Accuracy: 0.7350\n",
            "\n",
            "--- Epoch 8/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.13it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.1948 | Train Accuracy: 0.9231\n",
            "  Valid Loss: 0.6202 | Valid Accuracy: 0.7925\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 9/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.50it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1590 | Train Accuracy: 0.9537\n",
            "  Valid Loss: 0.4380 | Valid Accuracy: 0.8500\n",
            "✅ New best model saved with validation accuracy: 0.8500\n",
            "\n",
            "--- Epoch 10/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.09it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.1108 | Train Accuracy: 0.9663\n",
            "  Valid Loss: 0.4632 | Valid Accuracy: 0.8400\n",
            "\n",
            "--- Epoch 11/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.06it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.0860 | Train Accuracy: 0.9719\n",
            "  Valid Loss: 0.4872 | Valid Accuracy: 0.8400\n",
            "\n",
            "--- Epoch 12/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.55it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 15.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.0780 | Train Accuracy: 0.9731\n",
            "  Valid Loss: 0.4958 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 13/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00,  7.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.0645 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.4941 | Valid Accuracy: 0.8500\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 14/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:05<00:00,  4.99it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.0570 | Train Accuracy: 0.9819\n",
            "  Valid Loss: 0.4947 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 15/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.60it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.0631 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.4888 | Valid Accuracy: 0.8525\n",
            "✅ New best model saved with validation accuracy: 0.8525\n",
            "\n",
            "--- Epoch 16/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.33it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.0573 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.4788 | Valid Accuracy: 0.8625\n",
            "✅ New best model saved with validation accuracy: 0.8625\n",
            "\n",
            "--- Epoch 17/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.00it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.0559 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.4802 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 18/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.06it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0426 | Train Accuracy: 0.9888\n",
            "  Valid Loss: 0.4825 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 19/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.35it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 25.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.0499 | Train Accuracy: 0.9844\n",
            "  Valid Loss: 0.4866 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 20/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.50it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.0553 | Train Accuracy: 0.9794\n",
            "  Valid Loss: 0.4781 | Valid Accuracy: 0.8600\n",
            "Learning rate reduced from 1e-05 to 1.0000000000000002e-06\n",
            "\n",
            "--- Epoch 21/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.04it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.0389 | Train Accuracy: 0.9875\n",
            "  Valid Loss: 0.4856 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 22/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.69it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0518 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.4831 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 23/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0399 | Train Accuracy: 0.9881\n",
            "  Valid Loss: 0.4841 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 24/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.04it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.0450 | Train Accuracy: 0.9869\n",
            "  Valid Loss: 0.4835 | Valid Accuracy: 0.8575\n",
            "Learning rate reduced from 1.0000000000000002e-06 to 1.0000000000000002e-07\n",
            "\n",
            "--- Epoch 25/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.36it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0424 | Train Accuracy: 0.9856\n",
            "  Valid Loss: 0.4910 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 26/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0539 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.4842 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 27/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.08it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0332 | Train Accuracy: 0.9912\n",
            "  Valid Loss: 0.4827 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 28/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  9.03it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0508 | Train Accuracy: 0.9838\n",
            "  Valid Loss: 0.4878 | Valid Accuracy: 0.8550\n",
            "Learning rate reduced from 1.0000000000000002e-07 to 1.0000000000000004e-08\n",
            "\n",
            "--- Epoch 29/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.59it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 17.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0496 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.4936 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 30/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.09it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0475 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.5031 | Valid Accuracy: 0.8475\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation accuracy achieved: 0.8625\n",
            "Best model saved to 'best_model.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "v1 PREDICTION"
      ],
      "metadata": {
        "id": "bxxt5xZ3InjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model.pth'\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction.json' # IMPORTANT: Rename this with your team name\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64 # Can be larger for inference\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract index from filename (e.g., \"501.jpg\" -> 501)\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad(): # Disable gradient calculation for speed\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Apply sigmoid and threshold at 0.5 to get final predictions\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            # Decode label: 1 -> \"fake\", 0 -> \"real\"\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "# Sort predictions by index for a clean, ordered output file\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    # Print a sample of the output\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgoyA3NPp5tg",
        "outputId": "748904a5-8259-4d5b-adee-b7cf142eb9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading model from 'best_model.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:01<00:00,  6.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 TRAINING"
      ],
      "metadata": {
        "id": "k2dux5DfzMEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model_v2.pth' # Saving to a new file to avoid overwriting the original\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set with RandomErasing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)) # TECHNIQUE 3: Stronger Augmentation\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "df = pd.read_csv(MASTER_CSV_PATH)\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['target_label']\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # TECHNIQUE 1: Increased Dropout\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # TECHNIQUE 2: Added Weight Decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWRMKqlyp5rJ",
        "outputId": "fce98b07-ed03-458b-d6d7-ff5fea557139"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.72it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5674 | Train Accuracy: 0.7094\n",
            "  Valid Loss: 1.9166 | Valid Accuracy: 0.5150\n",
            "✅ New best model saved with validation accuracy: 0.5150\n",
            "\n",
            "--- Epoch 2/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.35it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.3883 | Train Accuracy: 0.8456\n",
            "  Valid Loss: 1.3026 | Valid Accuracy: 0.7425\n",
            "✅ New best model saved with validation accuracy: 0.7425\n",
            "\n",
            "--- Epoch 3/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3116 | Train Accuracy: 0.8744\n",
            "  Valid Loss: 0.4294 | Valid Accuracy: 0.8425\n",
            "✅ New best model saved with validation accuracy: 0.8425\n",
            "\n",
            "--- Epoch 4/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.98it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2712 | Train Accuracy: 0.8950\n",
            "  Valid Loss: 0.4314 | Valid Accuracy: 0.8625\n",
            "✅ New best model saved with validation accuracy: 0.8625\n",
            "\n",
            "--- Epoch 5/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.75it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2576 | Train Accuracy: 0.8944\n",
            "  Valid Loss: 0.4255 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 6/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.86it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2349 | Train Accuracy: 0.9087\n",
            "  Valid Loss: 0.4250 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 7/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.79it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2097 | Train Accuracy: 0.9331\n",
            "  Valid Loss: 0.4044 | Valid Accuracy: 0.8675\n",
            "✅ New best model saved with validation accuracy: 0.8675\n",
            "\n",
            "--- Epoch 8/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.08it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.2109 | Train Accuracy: 0.9163\n",
            "  Valid Loss: 0.3998 | Valid Accuracy: 0.8400\n",
            "\n",
            "--- Epoch 9/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.59it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1406 | Train Accuracy: 0.9494\n",
            "  Valid Loss: 0.4432 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 10/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.71it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.1778 | Train Accuracy: 0.9300\n",
            "  Valid Loss: 0.9059 | Valid Accuracy: 0.7250\n",
            "\n",
            "--- Epoch 11/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.76it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.1775 | Train Accuracy: 0.9450\n",
            "  Valid Loss: 0.3864 | Valid Accuracy: 0.8625\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 12/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.67it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.1167 | Train Accuracy: 0.9625\n",
            "  Valid Loss: 0.3554 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 13/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.09it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.0970 | Train Accuracy: 0.9681\n",
            "  Valid Loss: 0.3792 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 14/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.84it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.0778 | Train Accuracy: 0.9744\n",
            "  Valid Loss: 0.4053 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 15/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  5.22it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.0694 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3844 | Valid Accuracy: 0.8575\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 16/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.20it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.0681 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.3973 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 17/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.41it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 15.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.0792 | Train Accuracy: 0.9738\n",
            "  Valid Loss: 0.4031 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 18/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.91it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0787 | Train Accuracy: 0.9706\n",
            "  Valid Loss: 0.3994 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 19/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.88it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.0546 | Train Accuracy: 0.9825\n",
            "  Valid Loss: 0.4077 | Valid Accuracy: 0.8675\n",
            "Learning rate reduced from 1e-05 to 1.0000000000000002e-06\n",
            "\n",
            "--- Epoch 20/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.40it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 10.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.0692 | Train Accuracy: 0.9781\n",
            "  Valid Loss: 0.4166 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 21/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.21it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.0589 | Train Accuracy: 0.9812\n",
            "  Valid Loss: 0.4040 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 22/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.75it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0702 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.4011 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 23/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.73it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0782 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.4017 | Valid Accuracy: 0.8675\n",
            "Learning rate reduced from 1.0000000000000002e-06 to 1.0000000000000002e-07\n",
            "\n",
            "--- Epoch 24/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.83it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.0702 | Train Accuracy: 0.9788\n",
            "  Valid Loss: 0.4029 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 25/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  4.04it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0653 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.3992 | Valid Accuracy: 0.8600\n",
            "\n",
            "--- Epoch 26/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.74it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0793 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.4013 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 27/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.71it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 14.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0788 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.4098 | Valid Accuracy: 0.8625\n",
            "Learning rate reduced from 1.0000000000000002e-07 to 1.0000000000000004e-08\n",
            "\n",
            "--- Epoch 28/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.84it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 12.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0688 | Train Accuracy: 0.9769\n",
            "  Valid Loss: 0.4107 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Epoch 29/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:03<00:00,  3.94it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0753 | Train Accuracy: 0.9738\n",
            "  Valid Loss: 0.4082 | Valid Accuracy: 0.8650\n",
            "\n",
            "--- Epoch 30/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:02<00:00,  4.77it/s]\n",
            "Validation: 100%|██████████| 4/4 [00:00<00:00, 13.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0731 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.4066 | Valid Accuracy: 0.8675\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation accuracy achieved: 0.8675\n",
            "Best model saved to 'best_model_v2.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 PREDICTION"
      ],
      "metadata": {
        "id": "9aU23576Iqg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model_v2.pth' # <-- THE ONLY CHANGE NEEDED\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction_v2.json' # Saving to a new file\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture to match the one we trained\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # IMPORTANT: Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY8nQ3_HviCQ",
        "outputId": "2d4e0477-13aa-46c1-d72b-e2a5895eec76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model from 'best_model_v2.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:00<00:00, 24.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction_v2.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRATIFIED RANDOM SPLITTING /train_epoch_split.py script"
      ],
      "metadata": {
        "id": "x3qRyKUOhSS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model_v4_epoch_split.pth'\n",
        "NUM_EPOCHS = 30 # Let's run for 30 epochs\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Custom Dataset Definition (Same as before) ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms (Same as v2) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 3. Load Master DataFrame (ONCE) ---\n",
        "try:\n",
        "    df_master = pd.read_csv(MASTER_CSV_PATH)\n",
        "    print(f\"Loaded master dataset with {len(df_master)} samples.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found. Please run the data preparation script first.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 4. Model Definition (Same as v2) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Using our best 0.5 dropout\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 5. Loss Function, Optimizer (Same as v2) ---\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # With weight decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 6. Training Loop (with Per-Epoch Splitting) ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- THIS IS YOUR STRATEGY ---\n",
        "    # Re-split the data at the start of every epoch\n",
        "    print(f\"Creating new stratified 80/20 split for Epoch {epoch+1}...\")\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_master,\n",
        "        test_size=0.2,       # 80% training, 20% validation\n",
        "        random_state=epoch,  # Use epoch number as random_state to ensure a NEW split\n",
        "        stratify=df_master['target_label']\n",
        "    )\n",
        "\n",
        "    # Create new Datasets and DataLoaders for this epoch\n",
        "    train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "    val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    # --- END OF NEW LOGIC ---\n",
        "\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    # Note: This is now based on a \"noisy\" val_accuracy\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    # Save the best model\n",
        "    # Note: This is now saving the model that performed best\n",
        "    # on its *specific* 400-image validation set.\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with (noisy) validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Highest validation accuracy achieved on a single epoch split: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SopxdGUeccb9",
        "outputId": "0f44ef4a-3203-416c-bb49-64425f34b3f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded master dataset with 2000 samples.\n",
            "\n",
            "--- Epoch 1/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5439 | Train Accuracy: 0.7238\n",
            "  Valid Loss: 1.1387 | Valid Accuracy: 0.6950\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.6950\n",
            "\n",
            "--- Epoch 2/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.73it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.4091 | Train Accuracy: 0.8256\n",
            "  Valid Loss: 0.4036 | Valid Accuracy: 0.8400\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.8400\n",
            "\n",
            "--- Epoch 3/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.56it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3750 | Train Accuracy: 0.8500\n",
            "  Valid Loss: 0.3330 | Valid Accuracy: 0.8800\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.8800\n",
            "\n",
            "--- Epoch 4/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.67it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.3444 | Train Accuracy: 0.8519\n",
            "  Valid Loss: 0.3760 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 5/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.18it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2975 | Train Accuracy: 0.8894\n",
            "  Valid Loss: 0.2618 | Valid Accuracy: 0.9050\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9050\n",
            "\n",
            "--- Epoch 6/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.70it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2768 | Train Accuracy: 0.8906\n",
            "  Valid Loss: 0.2238 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 7/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.47it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2426 | Train Accuracy: 0.9181\n",
            "  Valid Loss: 0.4362 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 8/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.50it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.2844 | Train Accuracy: 0.8888\n",
            "  Valid Loss: 0.2821 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 9/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.38it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.2371 | Train Accuracy: 0.9163\n",
            "  Valid Loss: 0.2374 | Valid Accuracy: 0.9200\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9200\n",
            "\n",
            "--- Epoch 10/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.52it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.2502 | Train Accuracy: 0.9038\n",
            "  Valid Loss: 0.2498 | Valid Accuracy: 0.8900\n",
            "\n",
            "--- Epoch 11/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.77it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.2106 | Train Accuracy: 0.9244\n",
            "  Valid Loss: 0.1829 | Valid Accuracy: 0.9425\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9425\n",
            "\n",
            "--- Epoch 12/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 19.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.2077 | Train Accuracy: 0.9175\n",
            "  Valid Loss: 0.1995 | Valid Accuracy: 0.9250\n",
            "\n",
            "--- Epoch 13/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.1908 | Train Accuracy: 0.9287\n",
            "  Valid Loss: 0.1312 | Valid Accuracy: 0.9500\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9500\n",
            "\n",
            "--- Epoch 14/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.85it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.1610 | Train Accuracy: 0.9431\n",
            "  Valid Loss: 0.1942 | Valid Accuracy: 0.9275\n",
            "\n",
            "--- Epoch 15/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.87it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.1752 | Train Accuracy: 0.9331\n",
            "  Valid Loss: 0.1514 | Valid Accuracy: 0.9425\n",
            "\n",
            "--- Epoch 16/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  8.33it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.2033 | Train Accuracy: 0.9281\n",
            "  Valid Loss: 0.1782 | Valid Accuracy: 0.9350\n",
            "\n",
            "--- Epoch 17/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.45it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.1854 | Train Accuracy: 0.9356\n",
            "  Valid Loss: 0.1434 | Valid Accuracy: 0.9425\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 18/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.80it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.1228 | Train Accuracy: 0.9594\n",
            "  Valid Loss: 0.0600 | Valid Accuracy: 0.9825\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9825\n",
            "\n",
            "--- Epoch 19/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.85it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.1124 | Train Accuracy: 0.9575\n",
            "  Valid Loss: 0.0669 | Valid Accuracy: 0.9750\n",
            "\n",
            "--- Epoch 20/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.94it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 16.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.0939 | Train Accuracy: 0.9706\n",
            "  Valid Loss: 0.0510 | Valid Accuracy: 0.9850\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9850\n",
            "\n",
            "--- Epoch 21/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  6.51it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 14.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.0904 | Train Accuracy: 0.9688\n",
            "  Valid Loss: 0.0476 | Valid Accuracy: 0.9850\n",
            "\n",
            "--- Epoch 22/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.80it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 22.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0828 | Train Accuracy: 0.9744\n",
            "  Valid Loss: 0.0214 | Valid Accuracy: 0.9950\n",
            "✅ New best model saved with (noisy) validation accuracy: 0.9950\n",
            "\n",
            "--- Epoch 23/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.92it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0814 | Train Accuracy: 0.9712\n",
            "  Valid Loss: 0.0400 | Valid Accuracy: 0.9875\n",
            "\n",
            "--- Epoch 24/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.91it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.0770 | Train Accuracy: 0.9731\n",
            "  Valid Loss: 0.0297 | Valid Accuracy: 0.9950\n",
            "\n",
            "--- Epoch 25/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.56it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 18.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0580 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.0199 | Valid Accuracy: 0.9925\n",
            "\n",
            "--- Epoch 26/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 26...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.75it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0621 | Train Accuracy: 0.9750\n",
            "  Valid Loss: 0.0162 | Valid Accuracy: 0.9950\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 27/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.88it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 24.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0445 | Train Accuracy: 0.9844\n",
            "  Valid Loss: 0.0091 | Valid Accuracy: 1.0000\n",
            "✅ New best model saved with (noisy) validation accuracy: 1.0000\n",
            "\n",
            "--- Epoch 28/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 28...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.93it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0415 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.0246 | Valid Accuracy: 0.9900\n",
            "\n",
            "--- Epoch 29/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 29...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:03<00:00,  7.26it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0493 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.0096 | Valid Accuracy: 0.9975\n",
            "\n",
            "--- Epoch 30/30 ---\n",
            "Creating new stratified 80/20 split for Epoch 30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:02<00:00,  8.92it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 23.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0435 | Train Accuracy: 0.9844\n",
            "  Valid Loss: 0.0161 | Valid Accuracy: 0.9975\n",
            "\n",
            "--- Training Complete ---\n",
            "Highest validation accuracy achieved on a single epoch split: 1.0000\n",
            "Best model saved to 'best_model_v4_epoch_split.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFICATION"
      ],
      "metadata": {
        "id": "oiSrTURokY4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "# --- LOAD THE NEW CHAMPION MODEL ---\n",
        "MODEL_PATH = 'best_model_v4_epoch_split.pth'\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "print(f\"Loading new champion model from '{MODEL_PATH}'...\")\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval() # Set to eval mode\n",
        "\n",
        "# --- 2. Prepare STABLE Validation Data ---\n",
        "# We will use the *exact* same 80/20 split as our original v2 test\n",
        "try:\n",
        "    df = pd.read_csv(MASTER_CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Re-create the *exact* same 80/20 split using random_state=42\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42, # This is our stable, benchmark split\n",
        "    stratify=df['target_label']\n",
        ")\n",
        "print(f\"Loaded our stable validation set of {len(val_df)} images.\")\n",
        "\n",
        "# Create the custom dataset\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_dataset = ValidationDataset(val_df, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- 3. Generate Predictions on the STABLE Validation Set ---\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validating Champion Model\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds_numeric = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        labels_numeric = labels.cpu().numpy()\n",
        "\n",
        "        if preds_numeric.ndim == 0:\n",
        "            preds_numeric = [preds_numeric.item()]\n",
        "            labels_numeric = [labels_numeric.item()]\n",
        "\n",
        "        all_preds.extend(preds_numeric)\n",
        "        all_labels.extend(labels_numeric)\n",
        "\n",
        "# --- 4. Calculate Final \"True\" Accuracy ---\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "print(\"\\n--- FINAL VALIDATION COMPLETE ---\")\n",
        "print(f\"Original v2 Model (Stable Accuracy): 0.9000\")\n",
        "print(f\"New v4 Model (Stable Accuracy)   : {final_accuracy:.4f}\")\n",
        "\n",
        "if final_accuracy > 0.9000:\n",
        "    print(\"\\n✅✅✅ IT'S CONFIRMED! Your strategy worked.\")\n",
        "    print(\"The new model is officially better.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ The original v2 model remains the champion.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCgSRbyNccZI",
        "outputId": "77b03293-3ca5-4a8d-cccd-0ab9d8c3cf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading new champion model from 'best_model_v4_epoch_split.pth'...\n",
            "Loaded our stable validation set of 400 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating Champion Model: 100%|██████████| 7/7 [00:00<00:00, 21.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FINAL VALIDATION COMPLETE ---\n",
            "Original v2 Model (Stable Accuracy): 0.9000\n",
            "New v4 Model (Stable Accuracy)   : 0.9925\n",
            "\n",
            "✅✅✅ IT'S CONFIRMED! Your strategy worked.\n",
            "The new model is officially better.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict_final.py script"
      ],
      "metadata": {
        "id": "IL-SaIHdkbN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "# --- LOAD THE CHAMPION MODEL ---\n",
        "MODEL_PATH = 'best_model_v4_epoch_split.pth'\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction_FINAL.json'\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading final champion model from '{MODEL_PATH}'...\")\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, indices in tqdm(test_loader, desc=\"Generating Final Predictions\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        if preds.ndim == 0:\n",
        "            preds = [preds.item()]\n",
        "            indices = [indices.item()]\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Final submission saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77J7Z6OeRxn",
        "outputId": "56144316-f0e3-4c34-b70b-094207f55d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading final champion model from 'best_model_v4_epoch_split.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Final Predictions: 100%|██████████| 8/8 [00:00<00:00, 23.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Final submission saved to 'teamname_prediction_FINAL.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlKbFRDWdt5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sj25IwICh7An"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}