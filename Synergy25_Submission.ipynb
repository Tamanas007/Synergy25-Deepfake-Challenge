{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "unzipping"
      ],
      "metadata": {
        "id": "hQLQt6ftrMft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkq4iTtOp3nf",
        "outputId": "bc7d76ba-810e-4986-cdd9-5091c5b1cf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping '/content/drive/MyDrive/test.zip'...\n",
            "Successfully unzipped all files to 'hackathon_dataset'\n",
            "\n",
            "Unzipped contents:\n",
            "['test/493.png', 'test/482.png', 'test/487.png', 'test/479.png', 'test/453.png', 'test/489.png', 'test/483.png', 'test/459.png', 'test/486.png', 'test/496.png', 'test/488.png', 'test/468.png', 'test/474.png', 'test/471.png', 'test/454.png', 'test/498.png', 'test/480.png', 'test/481.png', 'test/450.png', 'test/466.png', 'test/460.png', 'test/500.png', 'test/473.png', 'test/467.png', 'test/497.png', 'test/476.png', 'test/484.png', 'test/463.png', 'test/449.png', 'test/465.png', 'test/485.png', 'test/461.png', 'test/478.png', 'test/472.png', 'test/469.png', 'test/464.png', 'test/499.png', 'test/462.png', 'test/470.png', 'test/451.png', 'test/456.png', 'test/448.png', 'test/452.png', 'test/457.png', 'test/458.png', 'test/455.png', 'test/491.png', 'test/475.png', 'test/495.png', 'test/492.png', 'test/442.png', 'test/494.png', 'test/490.png', 'test/391.png', 'test/441.png', 'test/412.png', 'test/408.png', 'test/477.png', 'test/444.png', 'test/402.png', 'test/415.png', 'test/431.png', 'test/439.png', 'test/447.png', 'test/432.png', 'test/392.png', 'test/443.png', 'test/397.png', 'test/395.png', 'test/416.png', 'test/422.png', 'test/403.png', 'test/423.png', 'test/433.png', 'test/440.png', 'test/446.png', 'test/394.png', 'test/430.png', 'test/399.png', 'test/445.png', 'test/407.png', 'test/419.png', 'test/424.png', 'test/390.png', 'test/401.png', 'test/393.png', 'test/413.png', 'test/414.png', 'test/425.png', 'test/421.png', 'test/434.png', 'test/427.png', 'test/418.png', 'test/426.png', 'test/410.png', 'test/438.png', 'test/409.png', 'test/436.png', 'test/400.png', 'test/435.png', 'test/429.png', 'test/417.png', 'test/396.png', 'test/398.png', 'test/389.png', 'test/406.png', 'test/428.png', 'test/420.png', 'test/404.png', 'test/437.png', 'test/411.png', 'test/405.png', 'test/364.png', 'test/370.png', 'test/388.png', 'test/332.png', 'test/357.png', 'test/335.png', 'test/343.png', 'test/359.png', 'test/362.png', 'test/371.png', 'test/361.png', 'test/373.png', 'test/385.png', 'test/386.png', 'test/368.png', 'test/375.png', 'test/327.png', 'test/349.png', 'test/340.png', 'test/325.png', 'test/378.png', 'test/337.png', 'test/377.png', 'test/367.png', 'test/363.png', 'test/369.png', 'test/374.png', 'test/350.png', 'test/372.png', 'test/382.png', 'test/346.png', 'test/330.png', 'test/354.png', 'test/345.png', 'test/358.png', 'test/326.png', 'test/379.png', 'test/338.png', 'test/347.png', 'test/342.png', 'test/344.png', 'test/329.png', 'test/376.png', 'test/334.png', 'test/360.png', 'test/384.png', 'test/331.png', 'test/351.png', 'test/355.png', 'test/348.png', 'test/365.png', 'test/341.png', 'test/356.png', 'test/366.png', 'test/339.png', 'test/324.png', 'test/381.png', 'test/333.png', 'test/323.png', 'test/380.png', 'test/383.png', 'test/387.png', 'test/300.png', 'test/279.png', 'test/336.png', 'test/308.png', 'test/353.png', 'test/277.png', 'test/318.png', 'test/328.png', 'test/352.png', 'test/301.png', 'test/266.png', 'test/267.png', 'test/285.png', 'test/269.png', 'test/292.png', 'test/295.png', 'test/322.png', 'test/273.png', 'test/290.png', 'test/281.png', 'test/303.png', 'test/268.png', 'test/286.png', 'test/305.png', 'test/307.png', 'test/260.png', 'test/265.png', 'test/298.png', 'test/274.png', 'test/293.png', 'test/263.png', 'test/262.png', 'test/312.png', 'test/261.png', 'test/310.png', 'test/294.png', 'test/296.png', 'test/311.png', 'test/304.png', 'test/297.png', 'test/282.png', 'test/275.png', 'test/271.png', 'test/314.png', 'test/313.png', 'test/316.png', 'test/289.png', 'test/288.png', 'test/284.png', 'test/299.png', 'test/302.png', 'test/283.png', 'test/309.png', 'test/320.png', 'test/270.png', 'test/264.png', 'test/291.png', 'test/276.png', 'test/321.png', 'test/317.png', 'test/278.png', 'test/306.png', 'test/205.png', 'test/201.png', 'test/200.png', 'test/272.png', 'test/287.png', 'test/206.png', 'test/219.png', 'test/213.png', 'test/242.png', 'test/315.png', 'test/228.png', 'test/319.png', 'test/280.png', 'test/252.png', 'test/248.png', 'test/241.png', 'test/223.png', 'test/245.png', 'test/224.png', 'test/247.png', 'test/243.png', 'test/256.png', 'test/237.png', 'test/246.png', 'test/255.png', 'test/238.png', 'test/207.png', 'test/216.png', 'test/230.png', 'test/232.png', 'test/234.png', 'test/251.png', 'test/259.png', 'test/236.png', 'test/211.png', 'test/239.png', 'test/212.png', 'test/254.png', 'test/258.png', 'test/240.png', 'test/225.png', 'test/231.png', 'test/233.png', 'test/203.png', 'test/217.png', 'test/227.png', 'test/199.png', 'test/222.png', 'test/210.png', 'test/235.png', 'test/215.png', 'test/244.png', 'test/250.png', 'test/220.png', 'test/226.png', 'test/204.png', 'test/249.png', 'test/229.png', 'test/209.png', 'test/221.png', 'test/214.png', 'test/218.png', 'test/257.png', 'test/253.png', 'test/208.png', 'test/192.png', 'test/146.png', 'test/143.png', 'test/202.png', 'test/148.png', 'test/150.png', 'test/160.png', 'test/198.png', 'test/163.png', 'test/183.png', 'test/153.png', 'test/169.png', 'test/194.png', 'test/185.png', 'test/187.png', 'test/171.png', 'test/165.png', 'test/147.png', 'test/155.png', 'test/197.png', 'test/173.png', 'test/172.png', 'test/141.png', 'test/145.png', 'test/170.png', 'test/176.png', 'test/186.png', 'test/195.png', 'test/158.png', 'test/175.png', 'test/188.png', 'test/177.png', 'test/164.png', 'test/167.png', 'test/157.png', 'test/161.png', 'test/193.png', 'test/156.png', 'test/182.png', 'test/159.png', 'test/151.png', 'test/179.png', 'test/180.png', 'test/191.png', 'test/154.png', 'test/162.png', 'test/178.png', 'test/189.png', 'test/168.png', 'test/144.png', 'test/174.png', 'test/166.png', 'test/190.png', 'test/196.png', 'test/98.png', 'test/85.png', 'test/152.png', 'test/149.png', 'test/142.png', 'test/105.png', 'test/81.png', 'test/87.png', 'test/108.png', 'test/78.png', 'test/102.png', 'test/132.png', 'test/100.png', 'test/181.png', 'test/117.png', 'test/119.png', 'test/90.png', 'test/135.png', 'test/95.png', 'test/125.png', 'test/184.png', 'test/93.png', 'test/122.png', 'test/89.png', 'test/131.png', 'test/113.png', 'test/128.png', 'test/114.png', 'test/104.png', 'test/126.png', 'test/84.png', 'test/88.png', 'test/101.png', 'test/94.png', 'test/106.png', 'test/116.png', 'test/121.png', 'test/112.png', 'test/124.png', 'test/115.png', 'test/91.png', 'test/107.png', 'test/86.png', 'test/99.png', 'test/137.png', 'test/130.png', 'test/83.png', 'test/96.png', 'test/92.png', 'test/109.png', 'test/110.png', 'test/77.png', 'test/129.png', 'test/82.png', 'test/127.png', 'test/136.png', 'test/103.png', 'test/111.png', 'test/79.png', 'test/97.png', 'test/134.png', 'test/133.png', 'test/120.png', 'test/140.png', 'test/123.png', 'test/34.png', 'test/118.png', 'test/80.png', 'test/70.png', 'test/19.png', 'test/55.png', 'test/138.png', 'test/48.png', 'test/139.png', 'test/24.png', 'test/51.png', 'test/54.png', 'test/45.png', 'test/42.png', 'test/14.png', 'test/71.png', 'test/38.png', 'test/63.png', 'test/65.png', 'test/73.png', 'test/44.png', 'test/20.png', 'test/62.png', 'test/49.png', 'test/66.png', 'test/36.png', 'test/13.png', 'test/56.png', 'test/67.png', 'test/43.png', 'test/27.png', 'test/47.png', 'test/61.png', 'test/23.png', 'test/35.png', 'test/28.png', 'test/22.png', 'test/52.png', 'test/46.png', 'test/15.png', 'test/18.png', 'test/26.png', 'test/30.png', 'test/21.png', 'test/37.png', 'test/25.png', 'test/39.png', 'test/32.png', 'test/41.png', 'test/40.png', 'test/64.png', 'test/72.png', 'test/76.png', 'test/57.png', 'test/60.png', 'test/33.png', 'test/29.png', 'test/17.png', 'test/31.png', 'test/74.png', 'test/59.png', 'test/69.png', 'test/58.png', 'test/75.png', 'test/50.png', 'test/53.png', 'test/16.png', 'test/4.png', 'test/7.png', 'test/12.png', 'test/3.png', 'test/68.png', 'test/6.png', 'test/9.png', 'test/2.png', 'test/1.png', 'test/10.png', 'test/8.png', 'test/11.png', 'test/5.png']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- 1. Set your paths ---\n",
        "\n",
        "# This is the name of the file you downloaded from the hackathon.\n",
        "# (e.g., \"Synergy25_dataset.zip\")\n",
        "zip_file_path = '/content/drive/MyDrive/test.zip'\n",
        "\n",
        "# This is the name of the folder where you want all the files to go.\n",
        "# (e.t., \"dataset/\")\n",
        "destination_folder = 'hackathon_dataset'\n",
        "\n",
        "# --- 2. Create the destination folder if it doesn't exist ---\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "    print(f\"Created directory: {destination_folder}\")\n",
        "\n",
        "# --- 3. Unzip the file ---\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        print(f\"Unzipping '{zip_file_path}'...\")\n",
        "        zip_ref.extractall(destination_folder)\n",
        "        print(f\"Successfully unzipped all files to '{destination_folder}'\")\n",
        "\n",
        "        # Optional: List the files you unzipped\n",
        "        print(\"\\nUnzipped contents:\")\n",
        "        print(zip_ref.namelist())\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file '{zip_file_path}' is not a valid zip file or is corrupted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{zip_file_path}' was not found.\")\n",
        "    print(\"Please make sure the file is in the same directory as this script, or provide the full path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "0xZekcdFuNKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import collections\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# It should contain the 5 subfolders ('real images', 'fake images', etc.)\n",
        "BASE_DATA_DIR = 'hackathon_dataset'\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/real_cifake_images')\n",
        "FAKE_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/fake_cifake_images')\n",
        "TEST_IMG_DIR = os.path.join(BASE_DATA_DIR, '/content/hackathon_dataset/test')\n",
        "REAL_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/real_cifake_preds.json', '/content/drive/MyDrive/real_cifake_preds.json') # Assuming file is named this\n",
        "FAKE_JSON_PATH = os.path.join(BASE_DATA_DIR, '/content/drive/MyDrive/fake_cifake_preds.json', '/content/drive/MyDrive/fake_cifake_preds.json') # Assuming file is named this\n",
        "\n",
        "print(\"--- Starting Dataset Verification ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 1: File Count Sanity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 1: File Count Sanity Check]\")\n",
        "try:\n",
        "    num_real_images = len(os.listdir(REAL_IMG_DIR))\n",
        "    num_fake_images = len(os.listdir(FAKE_IMG_DIR))\n",
        "    num_test_images = len(os.listdir(TEST_IMG_DIR))\n",
        "\n",
        "    print(f\"Found {num_real_images} images in 'real images' folder.\")\n",
        "    print(f\"Found {num_fake_images} images in 'fake images' folder.\")\n",
        "    print(f\"Found {num_test_images} images in 'test image' folder.\")\n",
        "\n",
        "    if num_real_images == 1000 and num_fake_images == 1000:\n",
        "        print(\"✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Image counts do not match the expected 1000/1000 split.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A folder was not found. Please check your paths. Details: {e}\")\n",
        "    exit() # Stop the script if basic folders are missing\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 2: The \"Imperfect Model\" Check (JSON Analysis)\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 2: JSON Prediction Analysis]\")\n",
        "try:\n",
        "    with open(REAL_JSON_PATH, 'r') as f:\n",
        "        real_json_data = json.load(f)\n",
        "    with open(FAKE_JSON_PATH, 'r') as f:\n",
        "        fake_json_data = json.load(f)\n",
        "\n",
        "    # Count predictions in the JSON for REAL images\n",
        "    real_json_counts = collections.Counter(item['prediction'] for item in real_json_data)\n",
        "    print(\"Proprietary model's predictions on REAL images:\")\n",
        "    print(f\"  - Predicted 'real': {real_json_counts.get('real', 0)}\")\n",
        "    print(f\"  - Predicted 'fake': {real_json_counts.get('fake', 0)}\")\n",
        "\n",
        "    # Count predictions in the JSON for FAKE images\n",
        "    fake_json_counts = collections.Counter(item['prediction'] for item in fake_json_data)\n",
        "    print(\"Proprietary model's predictions on FAKE images:\")\n",
        "    print(f\"  - Predicted 'fake': {fake_json_counts.get('fake', 0)}\")\n",
        "    print(f\"  - Predicted 'real': {fake_json_counts.get('real', 0)}\")\n",
        "\n",
        "    # --- The CRITICAL VERDICT ---\n",
        "    if real_json_counts.get('fake', 0) == 0 and fake_json_counts.get('real', 0) == 0:\n",
        "        print(\"✅ STATUS: The proprietary model is 'perfect' on the training set.\")\n",
        "        print(\"   Our task is a standard, balanced binary classification.\")\n",
        "    else:\n",
        "        print(\"⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\")\n",
        "        print(\"   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: A JSON file was not found. Please check your JSON file names and paths. Details: {e}\")\n",
        "    exit()\n",
        "except json.JSONDecodeError:\n",
        "    print(\"❌ ERROR: Could not parse a JSON file. It might be corrupted.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CHECK 3: Image Format & Integrity Check\n",
        "# ==============================================================================\n",
        "print(\"\\n[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\")\n",
        "image_sizes = set()\n",
        "image_modes = set()\n",
        "corrupted_files = []\n",
        "\n",
        "def check_images(directory, num_to_check=10):\n",
        "    files = os.listdir(directory)\n",
        "    for i, filename in enumerate(files):\n",
        "        if i >= num_to_check:\n",
        "            break\n",
        "        try:\n",
        "            with Image.open(os.path.join(directory, filename)) as img:\n",
        "                image_sizes.add(img.size)\n",
        "                image_modes.add(img.mode)\n",
        "        except Exception as e:\n",
        "            corrupted_files.append(os.path.join(directory, filename))\n",
        "\n",
        "try:\n",
        "    check_images(REAL_IMG_DIR)\n",
        "    check_images(FAKE_IMG_DIR)\n",
        "\n",
        "    print(f\"Found image sizes: {image_sizes}\")\n",
        "    print(f\"Found image modes (e.g., RGB, L): {image_modes}\")\n",
        "\n",
        "    if len(image_sizes) == 1:\n",
        "        print(\"✅ STATUS: All tested images have a consistent size.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying sizes. We will need to resize them all.\")\n",
        "\n",
        "    if len(image_modes) == 1 and 'RGB' in image_modes:\n",
        "        print(\"✅ STATUS: All tested images are in consistent 'RGB' mode.\")\n",
        "    else:\n",
        "        print(\"⚠️ WARNING: Images have varying modes (e.g., Grayscale 'L') or are not RGB.\")\n",
        "\n",
        "    if not corrupted_files:\n",
        "        print(\"✅ STATUS: No corrupted images found in the sample.\")\n",
        "    else:\n",
        "        print(f\"⚠️ WARNING: Found {len(corrupted_files)} corrupted images: {corrupted_files}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: An unexpected error occurred during image check. Details: {e}\")\n",
        "\n",
        "print(\"\\n--- Verification Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJMbRaNqp54Y",
        "outputId": "1f21973d-09b5-4024-dedf-b5d438252c7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Dataset Verification ---\n",
            "\n",
            "[CHECK 1: File Count Sanity Check]\n",
            "Found 1000 images in 'real images' folder.\n",
            "Found 1000 images in 'fake images' folder.\n",
            "Found 500 images in 'test image' folder.\n",
            "✅ STATUS: Correct number of training images found (1000 real, 1000 fake).\n",
            "\n",
            "[CHECK 2: JSON Prediction Analysis]\n",
            "Proprietary model's predictions on REAL images:\n",
            "  - Predicted 'real': 976\n",
            "  - Predicted 'fake': 24\n",
            "Proprietary model's predictions on FAKE images:\n",
            "  - Predicted 'fake': 988\n",
            "  - Predicted 'real': 12\n",
            "⚠️ STATUS: The proprietary model is 'imperfect'. It makes mistakes.\n",
            "   This is an imbalanced/noisy-label problem. Our goal is to MIMIC THESE MISTAKES.\n",
            "\n",
            "[CHECK 3: Image Integrity Check (testing a sample of 10 from each folder)]\n",
            "Found image sizes: {(32, 32)}\n",
            "Found image modes (e.g., RGB, L): {'RGB'}\n",
            "✅ STATUS: All tested images have a consistent size.\n",
            "✅ STATUS: All tested images are in consistent 'RGB' mode.\n",
            "✅ STATUS: No corrupted images found in the sample.\n",
            "\n",
            "--- Verification Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIPELINE"
      ],
      "metadata": {
        "id": "Upet8ScZvioy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# --- Configuration ---\n",
        "# This should be the path to the folder where you unzipped everything.\n",
        "# BASE_DATA_DIR = 'hackathon_dataset' # No longer needed with absolute paths\n",
        "\n",
        "# Define the paths to your folders and files\n",
        "REAL_IMG_DIR = '/content/hackathon_dataset/real_cifake_images'\n",
        "FAKE_IMG_DIR = '/content/hackathon_dataset/fake_cifake_images'\n",
        "REAL_JSON_PATH = '/content/drive/MyDrive/real_cifake_preds.json'\n",
        "FAKE_JSON_PATH = '/content/drive/MyDrive/fake_cifake_preds.json'\n",
        "\n",
        "# Output file name\n",
        "OUTPUT_CSV_PATH = 'master_labels.csv'\n",
        "\n",
        "def process_data(image_dir, json_path, data_list):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and an image directory, and populates a list with\n",
        "    image paths and their corresponding target labels.\n",
        "    \"\"\"\n",
        "    print(f\"Processing data from: {os.path.basename(json_path)}\")\n",
        "\n",
        "    # --- Load the JSON prediction data ---\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            predictions = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: JSON file not found at {json_path}. Please check the path and filename.\")\n",
        "        return False\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"❌ ERROR: Could not decode JSON from {json_path}. The file might be corrupted.\")\n",
        "        return False\n",
        "\n",
        "    # --- Create a dictionary for quick lookup: {index: prediction} ---\n",
        "    prediction_map = {item['index']: item['prediction'] for item in predictions}\n",
        "\n",
        "    # --- Iterate through images and create the master list ---\n",
        "    image_files = os.listdir(image_dir)\n",
        "    for filename in image_files:\n",
        "        # Assumes image filenames are like \"1.jpg\", \"2.png\", etc.\n",
        "        # We extract the number to use as the index.\n",
        "        try:\n",
        "            # Get the base name without extension (e.g., \"1\") and convert to integer\n",
        "            file_index = int(os.path.splitext(filename)[0])\n",
        "        except ValueError:\n",
        "            print(f\"⚠️ Warning: Could not parse index from filename '{filename}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        if file_index in prediction_map:\n",
        "            prediction_str = prediction_map[file_index]\n",
        "\n",
        "            # Encode labels: \"real\" -> 0, \"fake\" -> 1\n",
        "            target_label = 1 if prediction_str == 'fake' else 0\n",
        "\n",
        "            # Get the full path to the image\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            data_list.append({\n",
        "                'image_path': image_path,\n",
        "                'target_label': target_label\n",
        "            })\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: No prediction found in JSON for image index {file_index} ('{filename}').\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data preparation process.\"\"\"\n",
        "    print(\"--- Starting Step 1: Data Preparation ---\")\n",
        "\n",
        "    master_data_list = []\n",
        "\n",
        "    # Process the \"real\" images and their corresponding JSON predictions\n",
        "    if not process_data(REAL_IMG_DIR, REAL_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # Process the \"fake\" images and their corresponding JSON predictions\n",
        "    if not process_data(FAKE_IMG_DIR, FAKE_JSON_PATH, master_data_list):\n",
        "        return # Stop if there was an error\n",
        "\n",
        "    # --- Convert the list to a pandas DataFrame ---\n",
        "    if not master_data_list:\n",
        "        print(\"❌ ERROR: No data was processed. The master list is empty. Halting.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(master_data_list)\n",
        "\n",
        "    # --- Shuffle the DataFrame to mix real and fake samples ---\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # --- Save the final DataFrame to a CSV file ---\n",
        "    try:\n",
        "        df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "        print(f\"\\n✅ Success! Created master dataset with {len(df)} entries.\")\n",
        "        print(f\"   Saved to '{OUTPUT_CSV_PATH}'.\")\n",
        "\n",
        "        # Display the first few rows and the class distribution\n",
        "        print(\"\\n--- Dataset Preview ---\")\n",
        "        print(df.head())\n",
        "        print(\"\\n--- Final Label Distribution ---\")\n",
        "        print(df['target_label'].value_counts())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Could not save the CSV file. Details: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMztruRp52v",
        "outputId": "55e71ee7-0930-4096-f8a4-02cc520c3213"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 1: Data Preparation ---\n",
            "Processing data from: real_cifake_preds.json\n",
            "Processing data from: fake_cifake_preds.json\n",
            "\n",
            "✅ Success! Created master dataset with 2000 entries.\n",
            "   Saved to 'master_labels.csv'.\n",
            "\n",
            "--- Dataset Preview ---\n",
            "                                          image_path  target_label\n",
            "0  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "1  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "2  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "3  /content/hackathon_dataset/real_cifake_images/...             0\n",
            "4  /content/hackathon_dataset/fake_cifake_images/...             1\n",
            "\n",
            "--- Final Label Distribution ---\n",
            "target_label\n",
            "1    1012\n",
            "0     988\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model.pth'\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32 # Based on our verification step\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            # Return a dummy image and label if file is missing\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "try:\n",
        "    df = pd.read_csv(MASTER_CSV_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: '{MASTER_CSV_PATH}' not found. Please run the data preparation script first.\")\n",
        "    exit()\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,       # 80% training, 20% validation\n",
        "    random_state=42,\n",
        "    stratify=df['target_label'] # CRITICAL for maintaining label distribution\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1) # Output is a single value\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss() # Handles the sigmoid activation internally\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Announce LR change manually if it happens\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4H_tZWo5p5xk",
        "outputId": "2ccf752a-8266-44ec-bc60-e5ee6b92dc98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5283 | Train Accuracy: 0.7500\n",
            "  Valid Loss: 0.8017 | Valid Accuracy: 0.7275\n",
            "✅ New best model saved with validation accuracy: 0.7275\n",
            "\n",
            "--- Epoch 2/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.3482 | Train Accuracy: 0.8600\n",
            "  Valid Loss: 0.4447 | Valid Accuracy: 0.8375\n",
            "✅ New best model saved with validation accuracy: 0.8375\n",
            "\n",
            "--- Epoch 3/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3010 | Train Accuracy: 0.8900\n",
            "  Valid Loss: 0.4851 | Valid Accuracy: 0.8175\n",
            "\n",
            "--- Epoch 4/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2550 | Train Accuracy: 0.9062\n",
            "  Valid Loss: 0.5031 | Valid Accuracy: 0.8625\n",
            "✅ New best model saved with validation accuracy: 0.8625\n",
            "\n",
            "--- Epoch 5/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.2207 | Train Accuracy: 0.9094\n",
            "  Valid Loss: 0.4531 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 6/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2245 | Train Accuracy: 0.9237\n",
            "  Valid Loss: 0.4137 | Valid Accuracy: 0.8375\n",
            "\n",
            "--- Epoch 7/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2178 | Train Accuracy: 0.9175\n",
            "  Valid Loss: 0.4627 | Valid Accuracy: 0.8275\n",
            "\n",
            "--- Epoch 8/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.1714 | Train Accuracy: 0.9375\n",
            "  Valid Loss: 0.4884 | Valid Accuracy: 0.8550\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 9/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1414 | Train Accuracy: 0.9519\n",
            "  Valid Loss: 0.4771 | Valid Accuracy: 0.8450\n",
            "\n",
            "--- Epoch 10/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.21it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.0963 | Train Accuracy: 0.9681\n",
            "  Valid Loss: 0.4608 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 11/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.0762 | Train Accuracy: 0.9738\n",
            "  Valid Loss: 0.4601 | Valid Accuracy: 0.8650\n",
            "✅ New best model saved with validation accuracy: 0.8650\n",
            "\n",
            "--- Epoch 12/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.0747 | Train Accuracy: 0.9731\n",
            "  Valid Loss: 0.4915 | Valid Accuracy: 0.8475\n",
            "\n",
            "--- Epoch 13/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.0562 | Train Accuracy: 0.9831\n",
            "  Valid Loss: 0.4965 | Valid Accuracy: 0.8550\n",
            "\n",
            "--- Epoch 14/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.22it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.0559 | Train Accuracy: 0.9806\n",
            "  Valid Loss: 0.5094 | Valid Accuracy: 0.8625\n",
            "\n",
            "--- Epoch 15/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.0392 | Train Accuracy: 0.9856\n",
            "  Valid Loss: 0.5127 | Valid Accuracy: 0.8700\n",
            "✅ New best model saved with validation accuracy: 0.8700\n",
            "\n",
            "--- Epoch 16/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.0456 | Train Accuracy: 0.9844\n",
            "  Valid Loss: 0.5680 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 17/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.0242 | Train Accuracy: 0.9925\n",
            "  Valid Loss: 0.5692 | Valid Accuracy: 0.8500\n",
            "\n",
            "--- Epoch 18/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0435 | Train Accuracy: 0.9862\n",
            "  Valid Loss: 0.6004 | Valid Accuracy: 0.8575\n",
            "\n",
            "--- Epoch 19/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation:  57%|█████▋    | 4/7 [00:00<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3219666678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3219666678.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: Image not found at {img_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model.pth'\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction.json' # IMPORTANT: Rename this with your team name\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64 # Can be larger for inference\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract index from filename (e.g., \"501.jpg\" -> 501)\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad(): # Disable gradient calculation for speed\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Apply sigmoid and threshold at 0.5 to get final predictions\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            # Decode label: 1 -> \"fake\", 0 -> \"real\"\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "# Sort predictions by index for a clean, ordered output file\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    # Print a sample of the output\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgoyA3NPp5tg",
        "outputId": "748904a5-8259-4d5b-adee-b7cf142eb9fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading model from 'best_model.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:01<00:00,  6.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BETTER TRAINING"
      ],
      "metadata": {
        "id": "k2dux5DfzMEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MASTER_CSV_PATH = 'master_labels.csv'\n",
        "MODEL_SAVE_PATH = 'best_model_v2.pth' # Saving to a new file to avoid overwriting the original\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset Definition ---\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading images from the master CSV file.\"\"\"\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = int(self.dataframe.iloc[idx]['target_label'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image not found at {img_path}\")\n",
        "            return torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE), -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- 2. Data Transforms and Splitting ---\n",
        "# Define augmentations for the training set with RandomErasing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)) # TECHNIQUE 3: Stronger Augmentation\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the master CSV\n",
        "df = pd.read_csv(MASTER_CSV_PATH)\n",
        "\n",
        "# Stratified split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['target_label']\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition (ResNet18) ---\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify the final layer for our binary classification task\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # TECHNIQUE 1: Increased Dropout\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# --- 4. Loss Function, Optimizer, Scheduler ---\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # TECHNIQUE 2: Added Weight Decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1)\n",
        "\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train_preds = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train_preds += (preds == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total_train_samples\n",
        "    train_accuracy = correct_train_preds / total_train_samples\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val_preds = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val_preds += (preds == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / total_val_samples\n",
        "    val_accuracy = correct_val_preds / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Valid Loss: {val_loss:.4f} | Valid Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_accuracy)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "    if new_lr < old_lr:\n",
        "        print(f\"Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"✅ New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best model saved to '{MODEL_SAVE_PATH}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWRMKqlyp5rJ",
        "outputId": "9e2e62f9-3002-47f5-b495-b98eb3fed364"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training set size: 1600\n",
            "Validation set size: 400\n",
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5804 | Train Accuracy: 0.7150\n",
            "  Valid Loss: 0.9034 | Valid Accuracy: 0.6750\n",
            "✅ New best model saved with validation accuracy: 0.6750\n",
            "\n",
            "--- Epoch 2/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.4136 | Train Accuracy: 0.8313\n",
            "  Valid Loss: 0.4148 | Valid Accuracy: 0.8525\n",
            "✅ New best model saved with validation accuracy: 0.8525\n",
            "\n",
            "--- Epoch 3/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Summary:\n",
            "  Train Loss: 0.3411 | Train Accuracy: 0.8694\n",
            "  Valid Loss: 0.4791 | Valid Accuracy: 0.8100\n",
            "\n",
            "--- Epoch 4/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.18it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Summary:\n",
            "  Train Loss: 0.2832 | Train Accuracy: 0.8906\n",
            "  Valid Loss: 0.4779 | Valid Accuracy: 0.8325\n",
            "\n",
            "--- Epoch 5/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Summary:\n",
            "  Train Loss: 0.3115 | Train Accuracy: 0.8675\n",
            "  Valid Loss: 0.5542 | Valid Accuracy: 0.8025\n",
            "\n",
            "--- Epoch 6/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.25it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Summary:\n",
            "  Train Loss: 0.2539 | Train Accuracy: 0.8962\n",
            "  Valid Loss: 0.5614 | Valid Accuracy: 0.7925\n",
            "Learning rate reduced from 0.001 to 0.0001\n",
            "\n",
            "--- Epoch 7/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Summary:\n",
            "  Train Loss: 0.2314 | Train Accuracy: 0.9175\n",
            "  Valid Loss: 0.4048 | Valid Accuracy: 0.8625\n",
            "✅ New best model saved with validation accuracy: 0.8625\n",
            "\n",
            "--- Epoch 8/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Summary:\n",
            "  Train Loss: 0.1745 | Train Accuracy: 0.9387\n",
            "  Valid Loss: 0.4040 | Valid Accuracy: 0.8700\n",
            "✅ New best model saved with validation accuracy: 0.8700\n",
            "\n",
            "--- Epoch 9/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Summary:\n",
            "  Train Loss: 0.1593 | Train Accuracy: 0.9513\n",
            "  Valid Loss: 0.4073 | Valid Accuracy: 0.8850\n",
            "✅ New best model saved with validation accuracy: 0.8850\n",
            "\n",
            "--- Epoch 10/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Summary:\n",
            "  Train Loss: 0.1430 | Train Accuracy: 0.9487\n",
            "  Valid Loss: 0.4002 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 11/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Summary:\n",
            "  Train Loss: 0.1322 | Train Accuracy: 0.9525\n",
            "  Valid Loss: 0.4063 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 12/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Summary:\n",
            "  Train Loss: 0.1388 | Train Accuracy: 0.9481\n",
            "  Valid Loss: 0.4103 | Valid Accuracy: 0.8925\n",
            "✅ New best model saved with validation accuracy: 0.8925\n",
            "\n",
            "--- Epoch 13/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.18it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Summary:\n",
            "  Train Loss: 0.1155 | Train Accuracy: 0.9600\n",
            "  Valid Loss: 0.4096 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 14/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Summary:\n",
            "  Train Loss: 0.1171 | Train Accuracy: 0.9650\n",
            "  Valid Loss: 0.4021 | Valid Accuracy: 0.8975\n",
            "✅ New best model saved with validation accuracy: 0.8975\n",
            "\n",
            "--- Epoch 15/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Summary:\n",
            "  Train Loss: 0.1235 | Train Accuracy: 0.9563\n",
            "  Valid Loss: 0.4355 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 16/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Summary:\n",
            "  Train Loss: 0.1118 | Train Accuracy: 0.9600\n",
            "  Valid Loss: 0.4416 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 17/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Summary:\n",
            "  Train Loss: 0.1026 | Train Accuracy: 0.9675\n",
            "  Valid Loss: 0.4494 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 18/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Summary:\n",
            "  Train Loss: 0.0925 | Train Accuracy: 0.9694\n",
            "  Valid Loss: 0.4626 | Valid Accuracy: 0.8750\n",
            "Learning rate reduced from 0.0001 to 1e-05\n",
            "\n",
            "--- Epoch 19/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Summary:\n",
            "  Train Loss: 0.0907 | Train Accuracy: 0.9644\n",
            "  Valid Loss: 0.4395 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 20/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.14it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Summary:\n",
            "  Train Loss: 0.0921 | Train Accuracy: 0.9688\n",
            "  Valid Loss: 0.4508 | Valid Accuracy: 0.8775\n",
            "\n",
            "--- Epoch 21/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Summary:\n",
            "  Train Loss: 0.0731 | Train Accuracy: 0.9756\n",
            "  Valid Loss: 0.4440 | Valid Accuracy: 0.8800\n",
            "\n",
            "--- Epoch 22/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Summary:\n",
            "  Train Loss: 0.0798 | Train Accuracy: 0.9694\n",
            "  Valid Loss: 0.4484 | Valid Accuracy: 0.8825\n",
            "Learning rate reduced from 1e-05 to 1.0000000000000002e-06\n",
            "\n",
            "--- Epoch 23/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Summary:\n",
            "  Train Loss: 0.0864 | Train Accuracy: 0.9706\n",
            "  Valid Loss: 0.4544 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 24/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.23it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Summary:\n",
            "  Train Loss: 0.0924 | Train Accuracy: 0.9637\n",
            "  Valid Loss: 0.4461 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 25/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:20<00:00,  1.23it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Summary:\n",
            "  Train Loss: 0.0841 | Train Accuracy: 0.9731\n",
            "  Valid Loss: 0.4449 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 26/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Summary:\n",
            "  Train Loss: 0.0921 | Train Accuracy: 0.9681\n",
            "  Valid Loss: 0.4548 | Valid Accuracy: 0.8850\n",
            "Learning rate reduced from 1.0000000000000002e-06 to 1.0000000000000002e-07\n",
            "\n",
            "--- Epoch 27/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Summary:\n",
            "  Train Loss: 0.0971 | Train Accuracy: 0.9613\n",
            "  Valid Loss: 0.4511 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 28/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Summary:\n",
            "  Train Loss: 0.0736 | Train Accuracy: 0.9762\n",
            "  Valid Loss: 0.4528 | Valid Accuracy: 0.8825\n",
            "\n",
            "--- Epoch 29/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Summary:\n",
            "  Train Loss: 0.0821 | Train Accuracy: 0.9712\n",
            "  Valid Loss: 0.4503 | Valid Accuracy: 0.8850\n",
            "\n",
            "--- Epoch 30/30 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Summary:\n",
            "  Train Loss: 0.0786 | Train Accuracy: 0.9744\n",
            "  Valid Loss: 0.4484 | Valid Accuracy: 0.8875\n",
            "Learning rate reduced from 1.0000000000000002e-07 to 1.0000000000000004e-08\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation accuracy achieved: 0.8975\n",
            "Best model saved to 'best_model_v2.pth'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Update these paths if they are different in your environment\n",
        "TEST_IMG_DIR = '/content/hackathon_dataset/test'\n",
        "MODEL_PATH = 'best_model_v2.pth' # <-- THE ONLY CHANGE NEEDED\n",
        "OUTPUT_JSON_PATH = 'teamname_prediction_v2.json' # Saving to a new file\n",
        "\n",
        "# Model and data settings (must match the training script)\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Custom Dataset for Test Images ---\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for loading test images.\"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        index = int(os.path.splitext(filename)[0])\n",
        "        return image, index\n",
        "\n",
        "# --- 2. Load Model ---\n",
        "print(f\"Loading model from '{MODEL_PATH}'...\")\n",
        "# Re-create the model architecture to match the one we trained\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5), # IMPORTANT: Must match the saved model's architecture\n",
        "    nn.Linear(256, 1)\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at '{MODEL_PATH}'.\")\n",
        "    exit()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval() # CRITICAL: Set model to evaluation mode\n",
        "\n",
        "# --- 3. Prepare Test Data ---\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(root_dir=TEST_IMG_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Found {len(test_dataset)} images in the test directory.\")\n",
        "\n",
        "# --- 4. Generate Predictions ---\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, indices in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy().astype(int)\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "        for index, pred in zip(indices, preds):\n",
        "            prediction_str = \"fake\" if pred == 1 else \"real\"\n",
        "            predictions.append({\"index\": int(index), \"prediction\": prediction_str})\n",
        "\n",
        "# --- 5. Save Output JSON ---\n",
        "predictions.sort(key=lambda x: x['index'])\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"\\n✅ Success! Predictions saved to '{OUTPUT_JSON_PATH}'\")\n",
        "    print(\"\\n--- Prediction Sample ---\")\n",
        "    print(json.dumps(predictions[:5], indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not write JSON file. Details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY8nQ3_HviCQ",
        "outputId": "1cbe2563-ac1a-4fd4-aef5-66e62b50783b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading model from 'best_model_v2.pth'...\n",
            "Found 500 images in the test directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Success! Predictions saved to 'teamname_prediction_v2.json'\n",
            "\n",
            "--- Prediction Sample ---\n",
            "[\n",
            "    {\n",
            "        \"index\": 1,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 2,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 3,\n",
            "        \"prediction\": \"fake\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 4,\n",
            "        \"prediction\": \"real\"\n",
            "    },\n",
            "    {\n",
            "        \"index\": 5,\n",
            "        \"prediction\": \"fake\"\n",
            "    }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sz0g-6tNwLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xf95vL8DwLDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPUWYbHvyfbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxMtx4sryfYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNdD-sQIzPF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtSpwK4SzPyT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}